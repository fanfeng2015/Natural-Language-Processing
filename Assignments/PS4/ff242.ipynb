{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ff242.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "nmMeIZR4Bbuh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Semantic Parsing and Text-to-SQL with Recurrent Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "l-KLA5RdBws9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "__instructor__ = \"Dragomir Radev\"\n",
        "__authors__ = \"Tao Yu\", \"Angus Fong\", \"Suyi Li\", \"Alexander R. Fabbri\", \"Chris Hidey\"\n",
        "__version__ = \"Introduction to Natural Language Processing, Spring 2019\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aD4lCyqUFQUq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Contents\n",
        "\n",
        "0. Overview\n",
        "0. Environment Setup\n",
        "0. Introduction to NLP with Deep Learning\n",
        "0. Assignment Part 1: Introduction to Pytorch and Semantic Parsing\n",
        "0. Assignment Part 2: SQLNet for Text-to-SQL in Pytorch\n",
        "0. Assignment Part 3: SQLNet with Torchtext\n",
        "0. Submission"
      ]
    },
    {
      "metadata": {
        "id": "9UewMjA8HC0r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In this assignment, You will:\n",
        "\n",
        "* Learn to use some helpful tools such as Google Colab, Pytorch, Torchtext\n",
        "* Develop a neural model for semantic parsing and the text-to-SQL Spider task\n",
        "\n",
        "In order to make sure you understand every step in common NLP tasks, we provide some useful background information and show most of the code with detailed comments in this notebook. Thus, this assignment looks long, but you can skip some parts based on your own background (see Quick Search Tags below). Also, the coding part of this assignment should not be hard since we give you some examples to follow. The main idea of this assignment is to teach you about the entire pipeline for NLP with deep learning.\n",
        "\n",
        "__Sections of this document__\n",
        "\n",
        "Section A: Background\n",
        "\n",
        "1. Introduction to Google Colab\n",
        "2. Introduction to Semantic Parsing\n",
        "3. Introduction to Pytorch\n",
        "\n",
        "Section B: Implementation of Deep Learning for Semantic Parsing\n",
        "\n",
        "4. Implementing SQLNet for Text-to-SQL in Pytorch\n",
        "5. Implementing SQLNet with Torchtext\n",
        "\n",
        "__Grading Criteria__\n",
        "\n",
        "*  Short questions about basic concepts in Pytorch and the Spider task - ___20 points___\n",
        "    0. Basic concepts in Pytorch - ___15 points___\n",
        "    0. Write SQL in Spider task  - ___5 points___\n",
        "    \n",
        "* Implement SQLNet for Text-to-SQL in Pytorch - ___45 points___\n",
        "    0. Implement the orderby module in SQLNet – __15__ points\n",
        "    0. Compute the loss for the orderby module – __10__ points\n",
        "    0. Compute accuracy for the orderby module – __10__ points\n",
        "    0. Limitations of the SQLNet model on the Spider task – __10__ points\n",
        "    \n",
        "* Convert input pipeline for SQLNet using Torchtext - ___35 points___\n",
        "    0. Reconstruct inputs for SQLNet – __15__ points\n",
        "    0. Implement training process using Torchtext iterator – __20__ points\n",
        "    \n",
        "    \n",
        " __Quick Search Tags__\n",
        " \n",
        "* **TODO**: to help you find questions and code snippets you have to address.\n",
        "\n",
        "* We show all code on this notebook and add detailed comments for your quick reference. It can be too long to keep track of what is going on here, but don't be afraid! Please follow the tags below to know which parts you can just scan and which parts you need to read carefully.\n",
        "  0. **Note**: very important things you should pay attention to\n",
        "  0. **SCAN**: parts you can just quickly scan\n",
        "  0. **#READCODE**: important code functions you need to understand\n",
        "  0. need to read otherwise \n",
        " \n",
        " **Note:** Do NOT remove any these Quick Search Tags in this notebook, TAs will use them to quickly find your answers."
      ]
    },
    {
      "metadata": {
        "id": "jw7zkUNPOlIw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Environment Setup\n",
        "\n",
        "For this assignment, we will use Google Colab. It is a free cloud service based on Jupyter Notebooks that lets you use a free GPU. Basically, you can quickly create, upload, share, and even edit togther Jupyter notebooks.\n",
        "\n",
        "Please refer to [tutorial 1](https://course.fast.ai/start_colab.html) or [tutorial 2](https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c) for additional instructions on how to use Google Colab.\n",
        "\n",
        "#### Using GPU in Colab\n",
        "Before running anything, you need to tell Colab that you want to use a GPU: \n",
        "1. Go to __Runtime__ option on the top left\n",
        "2. Click __Change runtime type__\n",
        "3. Select \"Python 3\" for __Runtime type__ and \"GPU\" for __Hardward accelerator__\n",
        "4. Click __SAVE__ button\n",
        "\n",
        "Colab has popular libraries already installed such as Pytorch, TensorFlow, OpenCV and Keras. Let's get started and verify this:"
      ]
    },
    {
      "metadata": {
        "id": "IkA0xCW6VVjf",
        "colab_type": "code",
        "outputId": "6001f508-cc92-40b5-adce-2a8a50e7b328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Pytorch version is: \", torch.__version__)\n",
        "print(\"You are using: \", device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch version is:  1.0.1.post2\n",
            "You are using:  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lVkyTD2XWkxu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should be using CUDA with Pytorch 1.0.1.post2.\n",
        "\n",
        "**Note**: Of course, there are some limits. For example, it allows you access to a free GPU for only 10 hours at one time.\n",
        "\n",
        "**Note**:  Also, your downloaded files will NOT persist after the allocated instance is shutdown. To avoid this, you need to permit Colab instance to read and write files to your Google Drive. Use the following code snippet to mount your Google Drive:"
      ]
    },
    {
      "metadata": {
        "id": "PcFqBWwgWmTt",
        "colab_type": "code",
        "outputId": "7b95d801-107e-414e-8053-e1264dd4443a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xJnfDyCVRJLZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You need to click the link, copy the code on the page, paste it in the box, hit enter, and you’ll see the message \"Mounted at /content/gdrive\" when you’ve successfully mounted your drive. \n",
        "\n",
        "**TODO**: Let's download [`nlp_hw4`](https://drive.google.com/drive/folders/1_a_01EZfCUnuWnchEfRHZnLsWIYwka_k?usp=sharing) (unzip it), which contains the data and files which will be used in this assignment, and put it on your Google Drive. Set directory paths below:"
      ]
    },
    {
      "metadata": {
        "id": "htH63-_g_yrR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "ROOT_DIR = \"/content/gdrive/My Drive/nlp_hw4/\"\n",
        "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
        "GLOVE_DIR = os.path.join(ROOT_DIR, \"glove.6B.50d.txt\")\n",
        "SAVED_MODEL_DIR = os.path.join(ROOT_DIR, \"saved_models\")\n",
        "TABLE_PATH = os.path.join(DATA_DIR, \"tables.json\")\n",
        "TRAIN_PATH = os.path.join(DATA_DIR, \"train_spider.json\")\n",
        "DEV_PATH = os.path.join(DATA_DIR, \"dev.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5AbHTaYaYYOT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, please refer to [tutorial 1](https://course.fast.ai/start_colab.html) or [tutorial 2](https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c) for more details on how to use Google Colab.\n",
        "\n",
        " **Note**: You need to finish this assignment on Google Colab  because you will share your Google Colab notebook with TAs for grading.\n",
        " \n",
        " You should be able to import all packages below:"
      ]
    },
    {
      "metadata": {
        "id": "tx_wulJKZKin",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import io\n",
        "import sys\n",
        "import tqdm\n",
        "import json\n",
        "import datetime\n",
        "import argparse\n",
        "import numpy as np\n",
        "from itertools import chain, count\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import torch\n",
        "import torchtext.data\n",
        "import torchtext.vocab\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sys.path.append(ROOT_DIR)\n",
        "from utils import SelPredictor, GroupPredictor, lower_keys, to_batch_query"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oD6jXEoWV5fL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction to NLP with Deep Learning (Optional)\n",
        "\n",
        "**Note**: This section contains some useful links and resources on NLP with Deep Learning  for your reference in the future. For this assignment, we assume you already understand these concepts. You can **SCAN** this part if you want. \n",
        "\n",
        "Deep learning is currently a very popular topic in NLP. Deep learning approaches have very recently obtained state-of-the-art performance across many different NLP tasks. The appeal of deep learning is enhanced by the idea that tasks can be modeled without extensive feature engineering. The term “deep learning“ often refers to neural networks with many hidden layers. However, many people use the term for any neural network with non-linear transformations.\n",
        "\n",
        "The standard multi-layer neural network (also called [a multi-layer perceptron or MLP](http://deeplearning.net/tutorial/mlp.html)) looks something like this:\n",
        "\n",
        "\n",
        "<center><img src=\"https://i.ibb.co/7knDjRp/mlp.png\" alt=\"mlp\" align=\"middle\"></center>\n",
        "\n",
        "\n",
        "In this example, the input layer $x$ is of size $d_0=3$, the hidden layer $h$ is of size $d_1=4$, and the output layer $f(x)$ is of size $d_2=2$. In matrix notation:\n",
        "\n",
        "\\begin{equation*}\n",
        "f(x) = g_2 \\left( W_2 \\cdot g_1(W_1 \\cdot x + b_1) + b_2 \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "where $W_n\\in R^{d_n \\times d_{n-1}}$ and $b_n\\in R^{d_n}$ are the __parameters__ of the network for layer $n$ and $g_n$ is an __activation function__ for layer $n$. We learn the parameters by minimizing a __cost function__ using some __optimization method__. The __hyperparameters__ of the model are the dimensionality of the input and hidden layers and number of hidden nodes.\n",
        "\n",
        "Determining the network structure, activation function, cost function, and optimization method is part of the process of modeling a neural network. If the cost function is differentiable, we can take the gradient of the cost function with respect to the parameters and update the parameters using backpropagation (backpropagation is a very important step to build a good network model, [here](http://colah.github.io/posts/2015-08-Backprop/) is a nice, simple tutorial on backpropagation) and apply a gradient-based optimization technique. For this course, we will not be deriving gradients but rather using a library for [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (for a better understanding of deep learning you should derive and implement backpropagation rather than using an autograd library). Alternatively, there are many search-based algorithms for learning neural network parameters without differentiation.\n",
        "\n",
        "There are many gradient-based optimization algorithms (popular methods include [Adagrad](http://sebastianruder.com/optimizing-gradient-descent/index.html#adagrad), [Adam](http://sebastianruder.com/optimizing-gradient-descent/index.html#adam), and [RMSprop](http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop). Refer to [Wiki](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) or [here](http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms) for more details). [Adagrad](http://sebastianruder.com/optimizing-gradient-descent/index.html#adagrad) is the method used for this project and is an improved form of gradient descent. If you have taken machine learning, you are probably familiar with gradient descent. [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) is a basic optimization algorithm for finding the minimum of a function.\n",
        "\n",
        "In order to actually model a network, we still need to select an activation function and a cost function. An [activation function](https://en.wikipedia.org/wiki/Activation_function) is a non-linear function that allows the MLP to approximate any function. Common activation functions include the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function), [hyperbolic tangent](https://en.wikipedia.org/wiki/Hyperbolic_function#Hyperbolic_tangent), and [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) (ReLU or rectifier). The cost function is very task-specific. We may wish to minimize squared error or another difference metric. We may also want a probabilistic interpretation. In this framework we want to minimize the negative log likelihood of the training data. One common probabilistic cost function is the [softmax function](https://en.wikipedia.org/wiki/Softmax_function).\n",
        "\n",
        "Two common network structures in NLP are [recurrent](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) and [recursive](http://www.iro.umontreal.ca/~bengioy/talks/gss2012-YB6-NLP-recursive.pdf) neural networks. Recurrent neural networks are used for sequential input or time series. These networks have been used successfully in NLP for part-of-speech tagging and named entity recognition. Recursive neural networks are used for tree structures and have had success at parsing and sentiment analysis. The difference between these networks and the MLP is that they model “hidden states” at the current time or node that are a composition of all previous states. Previously in this class we studied HMMs, where the hidden states are discrete variables. In the recurrent neural network, the hidden state is a continuous variable.\n",
        "\n",
        "Here is a really good, vivid diagram of a recurrent neural network from [Stanford cs224u](http://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/master/nli.ipynb#Homework-4) (It shows many critical steps and implementation details in training any neural networks (not just recurrent ones) in NLP. This graph is very helpful in fully understanding this assignment.):\n",
        "\n",
        "<center><img src=\"https://i.ibb.co/NTCDZ7f/rnn.png\" alt=\"rnn\" width=\"450\"></center>\n",
        "\n",
        "The model definition of the above graph is as follows:\n",
        "\n",
        "\\begin{equation*}\n",
        "h_t = g_h \\left( W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "f(x_t) = g_o \\left( h_t \\cdot W_{hy} + b_o \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "where each hidden state $h_t$ is a composition of the previous state and $x_t$, the feature vector for element $x$ at time $t$. The parameters to be learned are the weight matrices $W$, the biases $b$, and an initial state $h_0$. For a specific application, consider that the elements $x_t$ are words and the outputs $f(x_t)$ are part-of-speech tags. We may choose the sigmoid function for $g_h$ and the softmax function for $g_o$ which will give us a probability distribution for a word at time $t$.\n",
        "\n",
        "In many NLP applications, the input vector will be a __word embedding__ (which we learned about in the previous assignments), a continuous representation of a word. These word embeddings may be pre-trained using methods such as [_GloVe_](http://nlp.stanford.edu/projects/glove/) or [_Word2vec_](https://code.google.com/archive/p/word2vec/) and then fed into the model. During training, there are several variations on how we treat word embeddings:\n",
        "\n",
        "0. The embeddings may be fixed and not changed during training.\n",
        "0. We may initialize them with pre-trained embeddings and learn them as parameters (possibly also adding a constraint that they are not allowed to vary too much).\n",
        "0. We may not use pre-trained embeddings at all and learn them as parameters with random initialization. \n",
        "\n",
        "Generally, the ad hoc wisdom is that for small-ish datasets it is better to use fixed embeddings.\n",
        "\n",
        "__Additional Resources__\n",
        "\n",
        "Many would still be confused by deep learning after this tutorial (it's really hard to understand!). Becuase this class is about NLP instead of deep learning, we can't provide a long tutorial here. You can check out the following materials for an in-depth understanding:\n",
        "\n",
        "* The links of key words in the above sections: most of them have nice tutorials with vivid examples and graphs instead of just theoretical interpretations.\n",
        "* [Great intro (with vivid diagrams) to LSTM/backpropagation/other ideas in networks](http://colah.github.io/)\n",
        "* [Using pre-trained word embeddings in networks](http://sebastianruder.com/word-embeddings-1/index.html)\n",
        "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "* For a full exploration of deep learning, check out NLP with deep learning classes at [Stanford](http://web.stanford.edu/class/cs224n/index.html) and [CMU](http://www.phontron.com/class/nn4nlp2019/)."
      ]
    },
    {
      "metadata": {
        "id": "LrcY94i9WQxU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assignment Part 1: Introduction to Pytorch and Semantic Parsing - 20 Points\n",
        "\n",
        "__a. Concepts and  Questions on Pytorch__\n",
        "\n",
        "We suppose that you are already familiar with basic concepts of Pytorch (used Pytorch in homework 2). At its core, PyTorch provides two main features: an n-dimensional Tensor, similar to Numpy but canrun on GPUs, and automatic differentiation for building and training neural networks. \n",
        "\n",
        "Please check out more details in [the Pytorch official tutorial](https://pytorch.org/tutorials/index.html). Especially, the following toturials are most helpful:\n",
        "\n",
        "* [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "* [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
        "* [What is torch.nn really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)\n",
        "* [Deep Learning for NLP with Pytorch](https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html)\n",
        "* [Chatbot Tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html)\n",
        "\n",
        "\n",
        "**Note**: To make this learning process even better, we put the full example code of all tutorials on the Colab. You can just copy [the notebooks](https://drive.google.com/drive/folders/1ltvDFeqMbV9FxnUfMWyD83X6iwfmjCWS?usp=sharing) to your directory and run the code directly on Colab!\n",
        "\n",
        "**TODO: Please answer the questions below (refer to the official tutorials) - 15 points**:\n",
        "1. Why do we run `model.eval()` during the evaluation?<br/>\n",
        " **YOUR ANSWER:** By default all modules in PyTorch are initialized to train mode, `model.eval()` explicitely indicates that code is switched to evaluation mode. Note that some layers may have different behaviors in train mode vs. evaluation mode, for example, bacth norm, or dropout.\n",
        "2. Why do we run `optimizer.zero_grad()` during the training?<br/>\n",
        " **YOUR ANSWER:** PyTorch by default accumulates gradients on subsequent backward passes. This is useful when training RNN, but in some other occasions where we don't want to accumulate the gradients (e.g., training a regular feed forward NN with mini-batches), `optimizer.zero_grad()` should be run during training.\n",
        "3. What do `loss.backward()` and `optimizer.step()` do?<br/>\n",
        " **YOUR ANSWER:** `loss.backward()` computes the gradients (of the loss w.r.t. the parameters), `optimizer.step()` performs the gradient descent (to actually update the parameters).\n",
        "\n",
        "__b. Introduction to Semantic Parsing and Text-to-SQL Spider Task – 5 points__\n",
        "\n",
        "___Semantic Parsing___ aims to map nautral language questions to executable programs such as logical forms, Python code, and SQL queries. The text-to-SQL task is one of the most important subtasks of semantic parsing. It converts natural language sentences to corresponding [SQL queries](https://www.w3schools.com/sql/sql_intro.asp).\n",
        "\n",
        "For exmaple, the goal of a text-to-SQL system is to automatically convert  ___\"What are the name and budget of the departments with average instructor salary greater than the overall average?\"___ into the SQL query below:\n",
        "\n",
        "```\n",
        "SELECT T2.name, T2.budget\n",
        "FROM instructor as T1 JOIN department as T2 ON T1.department_id = T2.id \n",
        "GROUP BY T1.department_id\n",
        "HAVING avg(T1.salary) > \n",
        "    (SELECT avg(salary) FROM instructor)\n",
        " ```\n",
        "\n",
        "**TODO**: Write a SQL query for \"What is the name of the department with the highest average instructor salary?\" – __5 points__<br/>\n",
        "**YOUR ANSWER:** \n",
        " \n",
        " ```\n",
        "SELECT T2.name\n",
        "FROM instructor as T1 JOIN department as T2 ON T1.department_id = T2.id\n",
        "GROUP BY T1.department_id\n",
        "ORDER BY avg(T1.salary) DESC\n",
        "LIMIT 1\n",
        " ```\n",
        "\n",
        "[___Spider___](https://yale-lily.github.io/spider) is a large-scale complex and cross-domain text-to-SQL challenge developed here at Yale. It has quickly become one of the most popular tasks in this filed. We already received several submissions from various research labs. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. It consists of 10k+ question-SQL pairs querying about 200 databases. In Spider, different complex SQL queries and databases appear in train and test sets. To do well on this task, systems must generalize well to not only new SQL queries but also new database schemas. Read more from [our EMNLP paper](https://arxiv.org/pdf/1809.08887.pdf) and [Medium blog](https://medium.com/@tao.yu/spider-one-more-step-towards-natural-language-interfaces-to-databases-62298dc6df3c)."
      ]
    },
    {
      "metadata": {
        "id": "CL1h078Jtshf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assignment Part 2: SQLNet for Text-to-SQL in Pytorch - 45 Points\n",
        "\n",
        "In this assignment, we are going to implement a neural text-to-SQL model [SQLNet](https://arxiv.org/pdf/1711.04436.pdf) for the Spider challenge. SQLNet was originally developed for a simpler text-to-SQL [WikiSQL task](https://einstein.ai/static/images/pages/research/seq2sql/seq2sql.pdf) (SQL queries in the WikiSQL dataset only include `SELECT` and `WHERE` clauses). It divides SQL generation into two parts/modules filling slots in **`SELECT`** (`$AGG $COLUMN` of `SELECT` in SQL sketch shown below) and **`WHERE`** (`$COLUMN $OP` of `WHERE` in SQL sketch) clauses:\n",
        "\n",
        "<center><img src=\"https://i.ibb.co/BGbcFNL/sqlnet-model.png\" alt=\"rnn\" width=\"450\"></center>\n",
        "\n",
        "We are going to extend SQLNet to the Spider task to include more complex `SELECT`, `GROUP BY` and `ORDER BY` modules.  Please read more about SQLNet model in [this paper](https://arxiv.org/pdf/1711.04436.pdf) (sepecially section 3.2 and 3.3).\n",
        "\n",
        "As you may noticed on [the leaderboard](https://yale-lily.github.io/spider), the performance of SQLNet on the Spider task is only 12.4% because of the difficulty of the task and the model's limitations. There are many research institutions which are working on this task with recently-introduced models (including more sophisticated [SyntaxSQL](https://arxiv.org/pdf/1810.05237.pdf) model developed by [the LILY lab](https://yale-lily.github.io/)). The best submission so far can achieve around 50% accuarcy (we will add their results on the leaderboard only after their papers get published). However, SQLNet is still a good example model to introduce you to the field of semantic parsing.\n",
        "\n",
        "In order to better understand the whole pipeline for common NLP tasks, you will be implementing many key steps including data preprocessing, a word embedding layer, batching, the key modules of SQLNet, loss functions, prediction postprocessing, and, finally, evaluation.\n",
        "\n",
        "#### 1. Data Loading and Preprocessing\n",
        "\n",
        "**Spider Data Content and Format**: Please read [the Spider Github page](https://github.com/taoyds/spider#data-content-and-format) to understand the Spider data content and format. Basically, there are two main raw inputs: the natural language questions in `train.json` and the database schema info (table and column names etc.) in `tables.json`. The gold labels are SQL queries. **Note**: In Spider task, we needn't to predict any values in SQL such as 42 in the example below (for reasons for this decision, please refer to the Spider paper).\n",
        "\n",
        "<center><img src=\"https://i.ibb.co/87ZZfvt/sqlnet.png\" alt=\"rnn\" width=\"500\"></center>\n",
        "\n",
        "First, we need to load these data files and convert raw SQL queries into target labels for each module in SQLNet. **Note**: Please go over the code to understand the data input structures.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "po5BtTk6M4If",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#READCODE!\n",
        "def process(sql_path, table_data):\n",
        "    '''\n",
        "    process data into inputs for modules in SQLNet\n",
        "    \n",
        "    ---Parameters---\n",
        "    \n",
        "    sql_path (str): dir to a input file that contains questions and SQL queries \n",
        "            such as train.json or dev.json \n",
        "    table_data (list of dicts): each dict in the list contains schema info \n",
        "            (table and column names etc.) of each database\n",
        "        \n",
        "    ---Returns---\n",
        "\n",
        "    output_sql (list of dicts): each dict in the list is a training data point \n",
        "            with question, database info, and target labels for each module \n",
        "            (agg/sel/SELECT, cond/WHERE, group/GROUP, order/ORDER) in SQLNet\n",
        "    '''\n",
        "    print(\"Loading data from %s\"%sql_path)\n",
        "    sql_data = []\n",
        "    #read and lowercase text\n",
        "    with open(sql_path) as inf:\n",
        "        data = lower_keys(json.load(inf))\n",
        "        sql_data += data\n",
        "    \n",
        "    #reformat table info\n",
        "    tables = {}\n",
        "    for i in range(len(table_data)):\n",
        "        table = table_data[i]\n",
        "        db_name = table['db_id']\n",
        "        tables[db_name] = table\n",
        "\n",
        "    #for each data point, store question and database info, and covert SQL into\n",
        "    #label inputs for each module in SQLNet\n",
        "    output_sql = []\n",
        "    for i in range(len(sql_data)):\n",
        "        sql = sql_data[i]\n",
        "        sql_one = {}\n",
        "\n",
        "        # add query metadata\n",
        "        sql_one['question'] = sql['question']\n",
        "        sql_one['question_tok'] = sql['question_toks']\n",
        "        sql_one['query'] = sql['query']\n",
        "        sql_one['query_tok'] = sql['query_toks']\n",
        "        sql_one['table_id'] = sql['db_id']\n",
        "        table = tables[sql['db_id']]\n",
        "        sql_one['column_names'] = table['column_names']\n",
        "        sql_one['col_org'] = table['column_names_original']\n",
        "        sql_one['table_org'] = table['table_names_original']\n",
        "        sql_one['fk_info'] = table['foreign_keys']\n",
        "        \n",
        "        # process aggregation/column selection labels for SELECT module\n",
        "        # agg labels are indices of ('none', 'max', 'min', 'count', 'sum', 'avg')\n",
        "        # sel labels are indices of column names\n",
        "        sql_one['agg'] = []\n",
        "        sql_one['sel'] = []\n",
        "        gt_sel = sql['sql']['select'][1]\n",
        "        if len(gt_sel) > 3: # limit to select at most 3 columns\n",
        "            gt_sel = gt_sel[:3]\n",
        "        for tup in gt_sel:\n",
        "            sql_one['agg'].append(tup[0])\n",
        "            sql_one['sel'].append(tup[1][1][1]) #GOLD for sel and agg\n",
        "        \n",
        "        # process where conditions\n",
        "        # op labels are indices of ('not', 'between', '=', '>', '<', '>=', '<=', \n",
        "        # '!=', 'in', 'like', 'is', 'exists')\n",
        "        sql_one['cond'] = []\n",
        "        gt_cond = sql['sql']['where']\n",
        "        if len(gt_cond) > 0:\n",
        "            conds = [gt_cond[x] for x in range(len(gt_cond)) if x % 2 == 0]\n",
        "            for cond in conds:\n",
        "                curr_cond = []\n",
        "                curr_cond.append(cond[2][1][1])\n",
        "                curr_cond.append(cond[1])\n",
        "                if cond[4] is not None:\n",
        "                    curr_cond.append([cond[3], cond[4]])\n",
        "                else:\n",
        "                    curr_cond.append(cond[3])\n",
        "                sql_one['cond'].append(curr_cond) #GOLD for COND [[col, op],[]]\n",
        "\n",
        "        # process group by / having\n",
        "        sql_one['group'] = [x[1] for x in sql['sql']['groupby']] #assume only one groupby\n",
        "        having_cond = []\n",
        "        if len(sql['sql']['having']) > 0:\n",
        "            gt_having = sql['sql']['having'][0] # currently only do first having condition\n",
        "            having_cond.append([gt_having[2][1][0]]) # aggregator\n",
        "            having_cond.append([gt_having[2][1][1]]) # column\n",
        "            having_cond.append([gt_having[1]]) # operator\n",
        "            if gt_having[4] is not None:\n",
        "                having_cond.append([gt_having[3], gt_having[4]])\n",
        "            else:\n",
        "                having_cond.append(gt_having[3])\n",
        "        else:\n",
        "            having_cond = [[], [], []]\n",
        "        sql_one['group'].append(having_cond) #GOLD for GROUP [[col1, col2, [agg, col, op]], [col, []]]\n",
        "\n",
        "        # process order by / limit\n",
        "        order_aggs = []\n",
        "        order_cols = []\n",
        "        sql_one['order'] = []\n",
        "        order_par = 4\n",
        "        gt_order = sql['sql']['orderby']\n",
        "        limit = sql['sql']['limit']\n",
        "        if len(gt_order) > 0:\n",
        "            order_aggs = [x[1][0] for x in gt_order[1][:1]] # limit to 1 order by\n",
        "            order_cols = [x[1][1] for x in gt_order[1][:1]]\n",
        "            if limit != None:\n",
        "                if gt_order[0] == 'asc':\n",
        "                    order_par = 0\n",
        "                else:\n",
        "                    order_par = 1\n",
        "            else:\n",
        "                if gt_order[0] == 'asc':\n",
        "                    order_par = 2\n",
        "                else:\n",
        "                    order_par = 3\n",
        "\n",
        "        sql_one['order'] = [order_aggs, order_cols, order_par] #GOLD for ORDER [[[agg], [col], [dat]], []]\n",
        "\n",
        "        # ingore intersect/except/union and nested queries because of \n",
        "        # limitations of SQLNet\n",
        "        \n",
        "        output_sql.append(sql_one)\n",
        "        \n",
        "    return output_sql\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vPcT1p_O1XiZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load and process train and development data. Now you can print some examples to see what the data looks like.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wPLCivVT1MxU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset_dir):\n",
        "    '''\n",
        "    load and process train and dev datasets\n",
        "    '''\n",
        "    with open(TABLE_PATH) as inf:\n",
        "        print(\"Loading data from %s\"%TABLE_PATH)\n",
        "        table_data= json.load(inf)\n",
        "    \n",
        "    train_sql_data = process(TRAIN_PATH, table_data)\n",
        "    val_sql_data = process(DEV_PATH, table_data)\n",
        "\n",
        "    return train_sql_data, val_sql_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QUlJUvjO1kPQ",
        "colab_type": "code",
        "outputId": "6e840380-745f-45ab-e91a-d25c5ed06e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "train_sql_data, val_sql_data = load_dataset(DATA_DIR)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data from /content/gdrive/My Drive/nlp_hw4/data/tables.json\n",
            "Loading data from /content/gdrive/My Drive/nlp_hw4/data/train_spider.json\n",
            "Loading data from /content/gdrive/My Drive/nlp_hw4/data/dev.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NTQmwQ7t1hxT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2. Word Embedding Layer\n",
        "\n",
        "Load pre-trained Glove word embedding `glove.6B.50d.txt`:"
      ]
    },
    {
      "metadata": {
        "id": "QHWK4LSb1iBU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_word_emb(file_name):\n",
        "    '''\n",
        "    load and convert pretrained glove embedding\n",
        "    \n",
        "    ---Returns---\n",
        "    ret (a dict): a embedding dict in which the key is a word and the value is \n",
        "          the embedding (np array) of that word.\n",
        "    '''\n",
        "    print ('Loading word embedding from %s'%file_name)\n",
        "    ret = {}\n",
        "    with open(file_name) as inf:\n",
        "        for idx, line in enumerate(inf):\n",
        "            info = line.strip().split(' ')\n",
        "            if info[0].lower() not in ret:\n",
        "                ret[info[0]] = np.array(list(map(lambda x:float(x), info[1:])))\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "77YDBIVx2jv4",
        "colab_type": "code",
        "outputId": "cdf1986e-434a-4d15-be55-3ec53d19eb34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "word_emb = load_word_emb(GLOVE_DIR)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word embedding from /content/gdrive/My Drive/nlp_hw4/glove.6B.50d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rvYzPWwF26EP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Build a word embedding layer containing functions that map words in questions and column names of databases into fixed-length embedding vectors."
      ]
    },
    {
      "metadata": {
        "id": "C24Ebw9p25gZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#READCODE!\n",
        "class WordEmbedding(nn.Module):\n",
        "    '''\n",
        "    word embedding layer mapping words in questions and column names into embeddings\n",
        "    '''\n",
        "    def __init__(self, word_emb, N_word):\n",
        "        '''\n",
        "        initialization method\n",
        "        \n",
        "        ---Parameters---\n",
        "        \n",
        "        word_emb (a dict): dict of loaded glove embeddings\n",
        "        N_word (int): embedding dimension\n",
        "        '''\n",
        "        super(WordEmbedding, self).__init__()\n",
        "        self.N_word = N_word\n",
        "        self.word_emb = word_emb\n",
        "\n",
        "    def gen_x_batch(self, q):\n",
        "        '''\n",
        "        map question inputs in each batch into embeddings\n",
        "        \n",
        "        ---Parameters---\n",
        "        \n",
        "        q (list of lists): each list include words in each question\n",
        "        \n",
        "        ---Returns---\n",
        "        \n",
        "        val_inp (tensor): question embedding input tensor for the model\n",
        "        val_len (list): each value in the list is the length of each question\n",
        "        '''\n",
        "        B = len(q)\n",
        "        val_embs = []\n",
        "        val_len = np.zeros(B, dtype=np.int64)\n",
        "        for i, one_q in enumerate(q):\n",
        "            #map each word into embedding\n",
        "            q_val = list(map(lambda x:self.word_emb.get(x, np.zeros(self.N_word, dtype=np.float32)), one_q))\n",
        "            #add start and end special tokens\n",
        "            val_embs.append([np.zeros(self.N_word, dtype=np.float32)] + q_val + [np.zeros(self.N_word, dtype=np.float32)])  #<BEG> and <END>\n",
        "            val_len[i] = 1 + len(q_val) + 1 \n",
        "        max_len = max(val_len)\n",
        "        #embedding output size is (batch size, max word length, embedding dim)\n",
        "        val_emb_array = np.zeros((B, max_len, self.N_word), dtype=np.float32)\n",
        "        for i in range(B):\n",
        "            for t in range(len(val_embs[i])):\n",
        "                val_emb_array[i,t,:] = val_embs[i][t]\n",
        "        val_inp = torch.from_numpy(val_emb_array).to(device)\n",
        "\n",
        "        return val_inp, val_len\n",
        "\n",
        "\n",
        "    def gen_col_batch(self, cols):\n",
        "        '''\n",
        "        map column names in each batch into embeddings\n",
        "        \n",
        "        ---Parameters---\n",
        "        \n",
        "        cols (list of lists of lists): list (batch of databases) of lists \n",
        "              (column names in each database) of lists (words in each column)\n",
        "        \n",
        "        ---Returns---\n",
        "        \n",
        "        name_inp (tensor): shape is (num of all column names in the batch, \n",
        "              embedding dim), each row in the tensor is embeddings of words \n",
        "              for each column name\n",
        "        name_len (list): each value in the list is the number of words in each \n",
        "              column name\n",
        "        col_len (list): each value in the list is the number of column names in \n",
        "              each database\n",
        "        '''\n",
        "        ret = []\n",
        "        col_len = np.zeros(len(cols), dtype=np.int64)\n",
        "        \n",
        "        #flatten cols into a list of lists of all column names in the batch\n",
        "        #[[list of cols[col1...] in one db], ...] -> [[col1], [col2],...]\n",
        "        names = []\n",
        "        for b, one_cols in enumerate(cols):\n",
        "            names = names + one_cols\n",
        "            col_len[b] = len(one_cols)\n",
        "        #map all column names in the batch into embeddings\n",
        "        name_inp, name_len = self.str_list_to_batch(names)\n",
        "        \n",
        "        return name_inp, name_len, col_len\n",
        "\n",
        "      \n",
        "    def str_list_to_batch(self, str_list):\n",
        "        \"\"\"\n",
        "        get a list var of wemb of words in each column name in current bactch\n",
        "        refer to gen_col_batch method\n",
        "        \"\"\"\n",
        "        B = len(str_list)\n",
        "\n",
        "        val_embs = []\n",
        "        val_len = np.zeros(B, dtype=np.int64)\n",
        "        for i, one_str in enumerate(str_list):\n",
        "            val = [self.word_emb.get(x, np.zeros(\n",
        "                  self.N_word, dtype=np.float32)) for x in one_str]\n",
        "            val_embs.append(val)\n",
        "            val_len[i] = len(val)\n",
        "        max_len = max(val_len)\n",
        "\n",
        "        val_emb_array = np.zeros(\n",
        "                (B, max_len, self.N_word), dtype=np.float32)\n",
        "        for i in range(B):\n",
        "            for t in range(len(val_embs[i])):\n",
        "                val_emb_array[i,t,:] = val_embs[i][t]\n",
        "        val_inp = torch.from_numpy(val_emb_array).to(device)\n",
        "\n",
        "        return val_inp, val_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3iGTvNqk3H7u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3. Running LSTM\n",
        "\n",
        "Below are functions to run an LSTM using packed sequence."
      ]
    },
    {
      "metadata": {
        "id": "X6J3T_Nz3FH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_lstm(lstm, inp, inp_len, hidden=None):\n",
        "    '''\n",
        "    run the LSTM using packed sequence\n",
        "    \n",
        "    ---Parameters---\n",
        "\n",
        "    lstm (object): a Pytorch LSTM module\n",
        "    inp (tensor): embedding input with shape (batch size, max x len, embed dim)\n",
        "    inp_len (list): number of words in input sequences in current batch\n",
        "    hidden (tensor): as initial hidden state\n",
        "\n",
        "    ---Returns---\n",
        "\n",
        "    ret_s (tensor): LSTM output containing the output features (h_t) from the \n",
        "          last layer of the LSTM, for each t\n",
        "    ret_h (tuple of tensors): (h_n, c_n), h_n: hidden states, c_n: cell states\n",
        "    '''\n",
        "    #running LSTM with packed sequence requires to first sort the input \n",
        "    #according to its length.\n",
        "    sort_perm = np.array(sorted(range(len(inp_len)),\n",
        "        key=lambda k:inp_len[k], reverse=True))\n",
        "    sort_inp_len = inp_len[sort_perm]\n",
        "    sort_perm_inv = np.argsort(sort_perm)\n",
        "    sort_perm = torch.LongTensor(sort_perm).to(device)\n",
        "    sort_perm_inv = torch.LongTensor(sort_perm_inv).to(device)\n",
        "    #reconstruct inp embedding input based on sorted indices\n",
        "    #pack padded batch of sequences for LSTM module\n",
        "    lstm_inp = nn.utils.rnn.pack_padded_sequence(inp[sort_perm],\n",
        "            sort_inp_len, batch_first=True)\n",
        "    if hidden is None:\n",
        "        lstm_hidden = None\n",
        "    else:\n",
        "        lstm_hidden = (hidden[0][:, sort_perm], hidden[1][:, sort_perm])\n",
        "    #run LSTM\n",
        "    sort_ret_s, sort_ret_h = lstm(lstm_inp, lstm_hidden)\n",
        "    #unpack a Tensor containing padded sequences of variable length.\n",
        "    ret_s = nn.utils.rnn.pad_packed_sequence(\n",
        "            sort_ret_s, batch_first=True)[0][sort_perm_inv]\n",
        "    ret_h = (sort_ret_h[0][:, sort_perm_inv], sort_ret_h[1][:, sort_perm_inv])\n",
        "    \n",
        "    return ret_s, ret_h\n",
        "\n",
        "\n",
        "def col_name_encode(name_inp_var, name_len, col_len, enc_lstm):\n",
        "    '''\n",
        "    run LSTM to encode the columns. The embedding of a column name is the last \n",
        "    state of its LSTM output.\n",
        "    \n",
        "    ---Parameters---\n",
        "\n",
        "    name_inp_var (tensor): shape is (num of all column names in the batch, \n",
        "          embedding dim), each row in the tensor is embeddings of words \n",
        "          for each column name\n",
        "    name_len (list): each value in the list is the number of words in each \n",
        "          column name\n",
        "    col_len (list): each value in the list is the number of column names in \n",
        "          each database\n",
        "    enc_lstm (object): a Pytorch LSTM module\n",
        "\n",
        "    ---Returns---\n",
        "\n",
        "    ret (tensor): LSTM output with shape (batch size, num of columns in each db,\n",
        "          embedding dim)\n",
        "    '''\n",
        "    #run LSTM between words in each column name\n",
        "    name_output, _ = run_lstm(enc_lstm, name_inp_var, name_len)\n",
        "    name_out = name_output[tuple(range(len(name_len))), name_len-1]\n",
        "    #shape: (batch size, num of columns in each db, embedding dim)\n",
        "    ret = torch.zeros(len(col_len), max(col_len), name_out.size()[1], device=device)\n",
        "    st = 0\n",
        "    for idx, cur_len in enumerate(col_len):\n",
        "        ret[idx, :cur_len] = name_out.data[st:st+cur_len]\n",
        "        st += cur_len\n",
        "        \n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r7C31uav3PvW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 4. Modules in SQLNet\n",
        "\n",
        "As we mentioned, SQLNet employs independent modules to predict slots in `SELECT`,  `WHERE`,  `GROUPBY`, and `ORDERBY` clauses, and then combines their predictions together to generate the final predicted SQL query.\n",
        "\n",
        "This section shows code for each module. Bascially, for each module, it does the following:\n",
        "0. runs LSTM forward pass on words of the question.\n",
        "0. runs LSTM forward pass on words of all column names in current batch.\n",
        "0. computes attention values between each column name and words of the question and gets the final question representation weighted the attentions.\n",
        "0. inputs the final question representation to nonlinearities to predict a probability distribution over all possible values for each slot in each clause."
      ]
    },
    {
      "metadata": {
        "id": "CshCRZYF3PUU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#READCODE!\n",
        "class CondPredictor(nn.Module):\n",
        "    '''\n",
        "    module to predict condition number, columns, and operators in WHERE condition.\n",
        "    '''\n",
        "    def __init__(self, N_word, N_h, N_depth):\n",
        "        '''\n",
        "        ---Parameters---\n",
        "        \n",
        "        N_word (int): embedding dimension\n",
        "        N_h (int): hidden size\n",
        "        N_depth (int): number of recurrent layers.\n",
        "        '''\n",
        "        super(CondPredictor, self).__init__()\n",
        "        self.N_h = N_h #hidden size\n",
        "        #initialize LSTM for encoding questions\n",
        "        self.q_lstm = nn.LSTM(input_size=N_word, hidden_size=N_h//2,\n",
        "                num_layers=N_depth, batch_first=True,\n",
        "                dropout=0.3, bidirectional=True)\n",
        "        #initialize LSTM for encoding column names\n",
        "        self.col_lstm = nn.LSTM(input_size=N_word, hidden_size=N_h//2,\n",
        "                num_layers=N_depth, batch_first=True,\n",
        "                dropout=0.3, bidirectional=True)\n",
        "        #initialize functions for condition number prediction\n",
        "        #limit the number of conditions to 0-5\n",
        "        self.q_num_att = nn.Linear(N_h, N_h)\n",
        "        self.col_num_out_q = nn.Linear(N_h, N_h)\n",
        "        self.col_num_out = nn.Sequential(nn.Tanh(), nn.Linear(N_h, 6))\n",
        "        #initialize functions for condition column prediction\n",
        "        self.q_att = nn.Linear(N_h, N_h)\n",
        "        self.col_out_q = nn.Linear(N_h, N_h)\n",
        "        self.col_out_c = nn.Linear(N_h, N_h)\n",
        "        self.col_out = nn.Sequential(nn.Tanh(), nn.Linear(N_h, 1))\n",
        "        #initialize functions for condition operation prediction\n",
        "        #the final op out dimension 12 is probs over all possible operators below \n",
        "        #('not', 'between', '=', '>', '<', '>=', '<=', '!=', 'in', 'like', 'is', 'exists')\n",
        "        self.op_att = nn.Linear(N_h, N_h)\n",
        "        self.op_out_q = nn.Linear(N_h, N_h)\n",
        "        self.op_out_c = nn.Linear(N_h, N_h)\n",
        "        self.op_out = nn.Sequential(nn.Tanh(), nn.Linear(N_h, 12)) \n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q_emb_var, q_len, col_emb_var, col_len, col_name_len, gt_cond):\n",
        "        '''\n",
        "        forward pass for condition module\n",
        "        \n",
        "        ---Parameters---\n",
        "        \n",
        "        q_emb_var (tensor): embeddings for question inputs,\n",
        "                shape: (batch size, max question len, embed dim)\n",
        "        q_len (list): number of words in question inputs in current batch\n",
        "        col_emb_var (tensor): embeddings for all column names in current batch,\n",
        "                shape: (num of all column names in the batch, embed dim), each \n",
        "                row in the tensor is embeddings of words for each column name\n",
        "        col_len (list): each value in the list is the number of column names in \n",
        "                each database\n",
        "        col_name_len (list): each value in the list is the number of words in each \n",
        "                column name\n",
        "                \n",
        "        ---Returns---\n",
        "        \n",
        "        score (tuple): includes col_num_score (batch size, 4), \n",
        "                col_score (batch size, max_col_len), op_score (batch size, 5, 12)?\n",
        "        '''\n",
        "        max_q_len = max(q_len)\n",
        "        max_col_len = max(col_len)\n",
        "        B = len(q_len)\n",
        "        #forward pass through LSTM for questions and column names\n",
        "        q_enc, _ = run_lstm(self.q_lstm, q_emb_var, q_len)\n",
        "        col_enc = col_name_encode(col_emb_var, col_name_len, col_len, self.col_lstm)\n",
        "\n",
        "        ## Predict condition number: 0-5 ## \n",
        "        # att_val_qc_num: (B, max_col_len, max_q_len)\n",
        "        # att_val_qc_num: attention before softmax tensor v btw q and col in SQLNet paper\n",
        "        att_val_qc_num = torch.bmm(col_enc, self.q_num_att(q_enc).transpose(1, 2))\n",
        "        #set prob for padded words to negative 100\n",
        "        for idx, num in enumerate(col_len):\n",
        "            if num < max_col_len:\n",
        "                att_val_qc_num[idx, num:, :] = -100\n",
        "        for idx, num in enumerate(q_len):\n",
        "            if num < max_q_len:\n",
        "                att_val_qc_num[idx, :, num:] = -100\n",
        "        # att_prob_qc_num: attention tensor w in SQLNet paper\n",
        "        att_prob_qc_num = self.softmax(att_val_qc_num.view((-1, max_q_len))).view(B, -1, max_q_len)\n",
        "        # q_weighted_num: (B, hid_dim) modified E_Q/Q\n",
        "        q_weighted_num = (q_enc.unsqueeze(1) * att_prob_qc_num.unsqueeze(3)).sum(2).sum(1)\n",
        "        # self.col_num_out: (B, 6)\n",
        "        # col_num_score: P_#col(K/Q) without softmax (because cros entropy loss \n",
        "        # in Pytorch does softmax) in SQLNet paper\n",
        "        col_num_score = self.col_num_out(self.col_num_out_q(q_weighted_num))\n",
        "\n",
        "        ## Predict columns in WHERE condition ## \n",
        "        # att_val_qc: attention before softmax tensor v btw q and col in SQLNet paper\n",
        "        att_val_qc = torch.bmm(col_enc, self.q_att(q_enc).transpose(1, 2))\n",
        "        for idx, num in enumerate(q_len):\n",
        "            if num < max_q_len:\n",
        "                att_val_qc[idx, :, num:] = -100\n",
        "        # att_prob_qc: attention tensor w in SQLNet paper\n",
        "        att_prob_qc = self.softmax(att_val_qc.view((-1, max_q_len))).view(B, -1, max_q_len)\n",
        "        # q_weighted: (B, max_col_len, hid_dim)\n",
        "        # q_weighted: E_Q/col question representation weighted by column attentions in SQLNet paper\n",
        "        q_weighted = (q_enc.unsqueeze(1) * att_prob_qc.unsqueeze(3)).sum(2)\n",
        "        # Compute prediction scores\n",
        "        # self.col_out.squeeze(): (B, max_col_len)\n",
        "        # col_score computed by P_where(col/Q) of equation 2 without softmax/sigmoid in SQLNet paper\n",
        "        col_score = self.col_out(self.col_out_q(q_weighted) + self.col_out_c(col_enc)).squeeze()\n",
        "        for idx, num in enumerate(col_len):\n",
        "            if num < max_col_len:\n",
        "                col_score[idx, num:] = -100\n",
        "                \n",
        "        # get select columns for op prediction because op depends on column\n",
        "        chosen_col_gt = []\n",
        "        if gt_cond is None: # if no gold cond column provided, use predicted cond\n",
        "            cond_nums = np.argmax(col_num_score.data.cpu().numpy(), axis=1)\n",
        "            col_scores = col_score.data.cpu().numpy()\n",
        "            chosen_col_gt = [list(np.argsort(-col_scores[b])[:cond_nums[b]]) for b in range(len(cond_nums))]\n",
        "        else:\n",
        "            chosen_col_gt = [[x[0] for x in one_gt_cond] for one_gt_cond in gt_cond]\n",
        "        # get embeddings for gold or predicted cond columns\n",
        "        col_emb = []\n",
        "        for b in range(B):\n",
        "            cur_col_emb = torch.stack([col_enc[b, x]\n",
        "                for x in chosen_col_gt[b]] + [col_enc[b, 0]] * (5 - len(chosen_col_gt[b])))\n",
        "            col_emb.append(cur_col_emb)\n",
        "        col_emb = torch.stack(col_emb)\n",
        "\n",
        "        ## Predict operators based on gold/predicted condition columns ## \n",
        "        op_att_val = torch.matmul(self.op_att(q_enc).unsqueeze(1),\n",
        "                col_emb.unsqueeze(3)).squeeze()\n",
        "        for idx, num in enumerate(q_len):\n",
        "            if num < max_q_len:\n",
        "                op_att_val[idx, :, num:] = -100\n",
        "        op_att = self.softmax(op_att_val.view(-1, max_q_len)).view(B, -1, max_q_len)\n",
        "        # E_Q/pred_or_gold_cols\n",
        "        q_weighted_op = (q_enc.unsqueeze(1) * op_att.unsqueeze(3)).sum(2)\n",
        "        # P_op(i/Q,col) in the paper\n",
        "        op_score = self.op_out(self.op_out_q(q_weighted_op) +\n",
        "                            self.op_out_c(col_emb)).squeeze()\n",
        "\n",
        "        score = (col_num_score, col_score, op_score)\n",
        "\n",
        "        return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJEqWKsIOnGN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TODO**: Please follow comments in the `WHERE` condition module to understand all steps listed above. You need to implement the `ORDERBY` module below. Consider `ORDER BY SUM(population) DESC LIMIT 1`."
      ]
    },
    {
      "metadata": {
        "id": "PM_bEMa3DVcb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TODO!\n",
        "class OrderPredictor(nn.Module):\n",
        "    '''\n",
        "    module to predict orderby column number, columns, and aggregations in ORDER clause.\n",
        "    '''\n",
        "    def __init__(self, N_word, N_h, N_depth):\n",
        "        super(OrderPredictor, self).__init__()\n",
        "        self.N_h = N_h\n",
        "       \n",
        "        #TODO:IMPLEMENT YOUR CODE BELOW\n",
        "        #initialize LSTM for encoding questions\n",
        "        self.q_lstm = nn.LSTM(input_size=N_word, hidden_size=N_h//2,\n",
        "                num_layers=N_depth, batch_first=True,\n",
        "                dropout=0.3, bidirectional=True)\n",
        "        \n",
        "        #initialize LSTM for encoding column names\n",
        "        self.col_lstm = nn.LSTM(input_size=N_word, hidden_size=N_h//2,\n",
        "                num_layers=N_depth, batch_first=True,\n",
        "                dropout=0.3, bidirectional=True)\n",
        "        \n",
        "        #initialize functions for orderby column number prediction\n",
        "        #only consider 0 or 1\n",
        "        self.q_num_att = nn.Linear(N_h, N_h)\n",
        "        self.col_num_out_q = nn.Linear(N_h, N_h)\n",
        "        self.col_num_out = nn.Sequential(nn.Tanh(), nn.Linear(N_h, 2))\n",
        "        \n",
        "        #initialize functions for orderby column prediction\n",
        "        self.q_att = nn.Linear(N_h, N_h)\n",
        "        self.col_out_q = nn.Linear(N_h, N_h)\n",
        "        self.col_out_c = nn.Linear(N_h, N_h)\n",
        "        self.col_out = nn.Sequential(nn.Tanh(), nn.Linear(N_h, 1))\n",
        "        \n",
        "        #initialize functions for orderby aggregation prediction\n",
        "        #here we assume there is at most one aggregator\n",
        "        #remember: agg ouput is probs over ('none', 'max', 'min', 'count', 'sum', 'avg')\n",
        "        self.agg_att = nn.Linear(N_h, N_h)\n",
        "        self.agg_out_q = nn.Linear(N_h, N_h)\n",
        "        self.agg_out_c = nn.Linear(N_h, N_h)\n",
        "        self.agg_out = nn.Sequential(nn.Tanh(), nn.Linear(N_h, 6)) \n",
        "        \n",
        "        #initialize functions for orderby parity\n",
        "        #possible values are (none, desc, asc, desc limit, asc limit)\n",
        "        self.par_att = nn.Linear(N_h, N_h)\n",
        "        self.par_out_q = nn.Linear(N_h, N_h)\n",
        "        self.par_out_c = nn.Linear(N_h, N_h)\n",
        "        self.par_out = nn.Sequential(nn.Tanh(), nn.Linear(N_h, 5)) \n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q_emb_var, q_len, col_emb_var, col_len, col_name_len):\n",
        "        '''\n",
        "        Forward pass for orderby prodection. \n",
        "        Note: we can use different ways to compute output probs for each module. \n",
        "        For simplicity, please follow exact the same ways as other modules do.\n",
        "        \n",
        "        ---Parameters---\n",
        "        \n",
        "        please refer to CondPredictor\n",
        "        '''\n",
        "        max_q_len = max(q_len)\n",
        "        max_col_len = max(col_len)\n",
        "        B = len(q_len)\n",
        "        \n",
        "        #forward pass through LSTM for questions and column names\n",
        "        q_enc, _ = run_lstm(self.q_lstm, q_emb_var, q_len)\n",
        "        col_enc = col_name_encode(col_emb_var, col_name_len, col_len, self.col_lstm)\n",
        "        \n",
        "        #TODO:IMPLEMENT YOUR CODE BELOW\n",
        "        ## Predict orderby column number 0 or 1 ##\n",
        "        att_val_qc_num = torch.bmm(col_enc, self.q_num_att(q_enc).transpose(1, 2))\n",
        "        for idx, num in enumerate(col_len):\n",
        "            if num < max_col_len:\n",
        "                att_val_qc_num[idx, num:, :] = -100\n",
        "        for idx, num in enumerate(q_len):\n",
        "            if num < max_q_len:\n",
        "                att_val_qc_num[idx, :, num:] = -100\n",
        "        att_prob_qc_num = self.softmax(att_val_qc_num.view((-1, max_q_len))).view(B, -1, max_q_len)\n",
        "        q_weighted_num = (q_enc.unsqueeze(1) * att_prob_qc_num.unsqueeze(3)).sum(2).sum(1)\n",
        "        col_num_score = self.col_num_out(self.col_num_out_q(q_weighted_num))\n",
        "        \n",
        "        ## Predict orderby column ##\n",
        "        att_val_qc = torch.bmm(col_enc, self.q_att(q_enc).transpose(1, 2))\n",
        "        for idx, num in enumerate(q_len):\n",
        "            if num < max_q_len:\n",
        "                att_val_qc[idx, :, num:] = -100\n",
        "        att_prob_qc = self.softmax(att_val_qc.view((-1, max_q_len))).view(B, -1, max_q_len)\n",
        "        q_weighted = (q_enc.unsqueeze(1) * att_prob_qc.unsqueeze(3)).sum(2)\n",
        "        col_score = self.col_out(self.col_out_q(q_weighted) + self.col_out_c(col_enc)).squeeze()\n",
        "        for idx, num in enumerate(col_len):\n",
        "            if num < max_col_len:\n",
        "                col_score[idx, num:] = -100\n",
        "        \n",
        "        ## Predict orderby aggregation ##\n",
        "        agg_att_val = torch.bmm(col_enc, self.agg_att(q_enc).transpose(1, 2))\n",
        "        for idx, num in enumerate(col_len):\n",
        "            if num < max_col_len:\n",
        "                agg_att_val[idx, num:, :] = -100\n",
        "        for idx, num in enumerate(q_len):\n",
        "            if num < max_q_len:\n",
        "                agg_att_val[idx, :, num:] = -100\n",
        "        agg_att = self.softmax(agg_att_val.view((-1, max_q_len))).view(B, -1, max_q_len)\n",
        "        q_weighted_agg = (q_enc.unsqueeze(1) * agg_att.unsqueeze(3)).sum(2).sum(1)\n",
        "        agg_score = self.agg_out(self.agg_out_q(q_weighted_agg))\n",
        "        \n",
        "        ## Predict if none, desc, asc , desc limit, asc limit ##\n",
        "        par_att_val = torch.bmm(col_enc, self.par_att(q_enc).transpose(1, 2))\n",
        "        for idx, num in enumerate(col_len):\n",
        "            if num < max_col_len:\n",
        "                par_att_val[idx, num:, :] = -100\n",
        "        for idx, num in enumerate(q_len):\n",
        "            if num < max_q_len:\n",
        "                par_att_val[idx, :, num:] = -100\n",
        "        par_att = self.softmax(par_att_val.view((-1, max_q_len))).view(B, -1, max_q_len)\n",
        "        q_weighted_par = (q_enc.unsqueeze(1) * par_att.unsqueeze(3)).sum(2).sum(1)\n",
        "        par_score = self.par_out(self.par_out_q(q_weighted_par))\n",
        "\n",
        "        score = (col_num_score, col_score, agg_score, par_score)\n",
        "\n",
        "        return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eEWyp5fO34W-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 5. SQLNet Model\n",
        "\n",
        "SQLNet main module calls all submodules to predict all components in SQL queries. It also computes the loss and accuracy of all submodules. For simplicity, we do not include functions that convert predictions into real SQL queries.\n",
        "\n",
        "\n",
        "**TODO**: Again,  the code is long, so you can just **SCAN** all code related to the `SELECT` and `GROUPBY` modules. Please follow comments for the code related to the `WHERE` module to understand what is going on there. You need to compute the loss and accuracy for the `ORDERBY` module."
      ]
    },
    {
      "metadata": {
        "id": "aeTrdaQc333N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#READCODE! But just SCAN all code related to SELECT and GROUPBY modules.\n",
        "#TODO!\n",
        "class SQLNet(nn.Module):\n",
        "    '''\n",
        "    SQLNet main module that calls all sub-modules\n",
        "    '''\n",
        "    def __init__(self, word_emb, N_word, N_h=120, N_depth=2):\n",
        "        super(SQLNet, self).__init__()\n",
        "        self.N_h = N_h\n",
        "        self.N_depth = N_depth\n",
        "\n",
        "        self.max_col_num = 45\n",
        "        self.max_tok_num = 200\n",
        "        self.COND_OPS = ['EQL', 'GT', 'LT']\n",
        "\n",
        "        self.embed_layer = WordEmbedding(word_emb, N_word)\n",
        "\n",
        "        #select predictor initialization\n",
        "        self.sel_pred = SelPredictor(N_word, N_h, N_depth)\n",
        "        #where condition predictor initialization\n",
        "        self.cond_pred = CondPredictor(N_word, N_h, N_depth)\n",
        "        #groupby predictor initialization\n",
        "        self.group_pred = GroupPredictor(N_word, N_h, N_depth)\n",
        "        #orderby predictor initialization\n",
        "        self.order_pred = OrderPredictor(N_word, N_h, N_depth)\n",
        "        \n",
        "        #loss function\n",
        "        self.CE = nn.CrossEntropyLoss()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, q, col, gt_cond=None, gt_sel=None):\n",
        "        '''\n",
        "        forward pass for all submodules in SQLNet model\n",
        "        \n",
        "        ---Parameters---\n",
        "        \n",
        "        q (list of lists): each list include words in each question\n",
        "        cols (list of lists of lists): list (batch of databases) of lists \n",
        "              (column names in each database) of lists (words in each column)\n",
        "        gt_cond(list of lists): list of gold columns in condition, it is None during testing \n",
        "        gt_sel (list of lists): list of gold columns in select, it is None during testing\n",
        "        \n",
        "        ---Returns---\n",
        "        \n",
        "        scores (tuple): prediction scores for each submodule\n",
        "        \n",
        "        '''\n",
        "        B = len(q)\n",
        "\n",
        "        sel_score = None\n",
        "        cond_score = None\n",
        "        group_score = None\n",
        "        order_score = None\n",
        "        #map words into embeddings\n",
        "        x_emb_var, x_len = self.embed_layer.gen_x_batch(q)\n",
        "        col_inp_var, col_name_len, col_len = self.embed_layer.gen_col_batch(col)\n",
        "        max_x_len = max(x_len)\n",
        "        #run forward pass for each submodule\n",
        "        sel_score = self.sel_pred(x_emb_var, x_len, col_inp_var, col_len, col_name_len, gt_sel=gt_sel)\n",
        "        cond_score = self.cond_pred(x_emb_var, x_len, col_inp_var, col_len, col_name_len, gt_cond=gt_cond)\n",
        "        group_score = self.group_pred(x_emb_var, x_len, col_inp_var, col_len, col_name_len)\n",
        "        order_score = self.order_pred(x_emb_var, x_len, col_inp_var, col_len, col_name_len)\n",
        "\n",
        "        scores = (sel_score, cond_score, group_score, order_score)\n",
        "        \n",
        "        return scores\n",
        "\n",
        "\n",
        "    def loss(self, score, truth_num):\n",
        "        '''\n",
        "        compute loss for each submodule prediction\n",
        "        \n",
        "        ---Parameters---\n",
        "        \n",
        "        score (tuple): scores returned by the forward pass\n",
        "        truth_num (list of tuples): each tuple in the list contains detailed gold\n",
        "              labels. refer to ans_seq in to_batch_seq function\n",
        "        \n",
        "        ---Returns---\n",
        "        \n",
        "        loss (scalar tensor): loss of all submodule\n",
        "        '''\n",
        "        sel_score, cond_score, group_score, order_score = score\n",
        "\n",
        "        sel_num_score, sel_col_score, agg_num_score, agg_op_score = sel_score\n",
        "        cond_num_score, cond_col_score, cond_op_score = cond_score\n",
        "        gby_num_score, gby_score, hv_score, hv_col_score, hv_agg_score, hv_op_score = group_score\n",
        "        ody_num_score, ody_col_score, ody_agg_score, ody_par_score = order_score\n",
        "\n",
        "        B = len(truth_num)\n",
        "        loss = 0\n",
        "        \n",
        "        #----------loss for sel_pred -------------#\n",
        "\n",
        "        # loss for sel agg # and sel agg\n",
        "        for b in range(len(truth_num)):\n",
        "            curr_col = truth_num[b][1][0]\n",
        "            curr_col_num_aggs = 0\n",
        "            gt_aggs_num = []\n",
        "            for i, col in enumerate(truth_num[b][1]):\n",
        "                if col != curr_col:\n",
        "                    gt_aggs_num.append(curr_col_num_aggs)\n",
        "                    curr_col = col\n",
        "                    curr_col_num_aggs = 0\n",
        "                if truth_num[b][0][i] != 0:\n",
        "                    curr_col_num_aggs += 1\n",
        "            gt_aggs_num.append(curr_col_num_aggs)\n",
        "            agg_num_truth_var = torch.from_numpy(np.array(gt_aggs_num)).to(device) #supposed to be gt # of aggs\n",
        "            agg_num_pred = agg_num_score[b, :truth_num[b][5]] # supposed to be gt # of select columns\n",
        "            loss += (self.CE(agg_num_pred, agg_num_truth_var) \\\n",
        "                    / len(truth_num))\n",
        "            # loss for sel agg prediction\n",
        "            T = 6 #num agg ops\n",
        "            truth_prob = np.zeros((truth_num[b][5], T), dtype=np.float32)\n",
        "            gt_agg_by_sel = []\n",
        "            curr_sel_aggs = []\n",
        "            curr_col = truth_num[b][1][0]\n",
        "            col_counter = 0\n",
        "            for i, col in enumerate(truth_num[b][1]):\n",
        "                if col != curr_col:\n",
        "                    gt_agg_by_sel.append(curr_sel_aggs)\n",
        "                    curr_col = col\n",
        "                    col_counter += 1\n",
        "                    curr_sel_aggs = [truth_num[b][0][i]]\n",
        "                    truth_prob[col_counter][curr_sel_aggs] = 1\n",
        "                else:\n",
        "                    curr_sel_aggs.append(truth_num[b][0][i])\n",
        "                    truth_prob[col_counter][curr_sel_aggs] = 1\n",
        "            agg_op_truth_var = torch.from_numpy(truth_prob).to(device)\n",
        "            agg_op_prob = self.sigm(agg_op_score[b, :truth_num[b][5]])\n",
        "            agg_bce_loss = -torch.mean( 3*(agg_op_truth_var * \\\n",
        "                    torch.log(agg_op_prob+1e-10)) + \\\n",
        "                    (1-agg_op_truth_var) * torch.log(1-agg_op_prob+1e-10) )\n",
        "            loss += agg_bce_loss / len(truth_num)\n",
        "\n",
        "        #Evaluate the number of select columns\n",
        "        sel_num_truth = list(map(lambda x: x[5]-1, truth_num)) #might need to be the length of the set of columms\n",
        "        sel_num_truth_var = torch.from_numpy(np.array(sel_num_truth)).to(device)\n",
        "        loss += self.CE(sel_num_score, sel_num_truth_var)\n",
        "        # Evaluate the select columns\n",
        "        T = len(sel_col_score[0])\n",
        "        truth_prob = np.zeros((B, T), dtype=np.float32)\n",
        "        for b in range(B):\n",
        "            truth_prob[b][truth_num[b][1]] = 1\n",
        "        sel_col_truth_var = torch.from_numpy(truth_prob).to(device)\n",
        "        sel_col_prob = self.sigm(sel_col_score)\n",
        "         #weighted negative log-likelihood loss(col, Q, y) in SQLNet paper\n",
        "        sel_bce_loss = -torch.mean( 3*(sel_col_truth_var * \\\n",
        "                torch.log(sel_col_prob+1e-10)) + \\\n",
        "                (1-sel_col_truth_var) * torch.log(1-sel_col_prob+1e-10) )\n",
        "        loss += sel_bce_loss\n",
        "        \n",
        "        #----------------loss for cond_pred--------------------#\n",
        "        \n",
        "        #Evaluate the number of conditions, cross entropy loss used\n",
        "        cond_num_truth = list(map(lambda x:x[2], truth_num))\n",
        "        cond_num_truth_var = torch.from_numpy(np.array(cond_num_truth)).to(device)\n",
        "        loss += self.CE(cond_num_score, cond_num_truth_var)\n",
        "        #Evaluate the columns of conditions\n",
        "        T = len(cond_col_score[0])\n",
        "        truth_prob = np.zeros((B, T), dtype=np.float32)\n",
        "        for b in range(B):\n",
        "            if len(truth_num[b][3]) > 0:\n",
        "                truth_prob[b][list(truth_num[b][3])] = 1\n",
        "\n",
        "        cond_col_truth_var = torch.from_numpy(truth_prob).to(device)\n",
        "        cond_col_prob = self.sigm(cond_col_score)\n",
        "        #weighted negative log-likelihood bce loss(col, Q, y) in SQLNet paper\n",
        "        bce_loss = -torch.mean( 3*(cond_col_truth_var * \\\n",
        "                torch.log(cond_col_prob+1e-10)) + \\\n",
        "                (1-cond_col_truth_var) * torch.log(1-cond_col_prob+1e-10) )\n",
        "        loss += bce_loss\n",
        "        #Evaluate the operator of conditions\n",
        "        for b in range(len(truth_num)):\n",
        "            if len(truth_num[b][4]) == 0:\n",
        "                continue\n",
        "            cond_op_truth_var = torch.from_numpy(np.array(truth_num[b][4])).to(device)\n",
        "            cond_op_pred = cond_op_score[b, :len(truth_num[b][4])]\n",
        "            loss += (self.CE(cond_op_pred, cond_op_truth_var) \\\n",
        "                    / len(truth_num))\n",
        "          \n",
        "        # -----------loss for group_pred -------------- #\n",
        "        \n",
        "        # Evaluate the number of group by columns\n",
        "        gby_num_truth = list(map(lambda x: x[7], truth_num))\n",
        "        gby_num_truth_var = torch.from_numpy(np.array(gby_num_truth)).to(device)\n",
        "        loss += self.CE(gby_num_score, gby_num_truth_var)\n",
        "        # Evaluate the group by columns\n",
        "        T = len(gby_score[0])\n",
        "        truth_prob = np.zeros((B, T), dtype=np.float32)\n",
        "        for b in range(B):\n",
        "            if len(truth_num[b][6]) > 0:\n",
        "                truth_prob[b][list(truth_num[b][6])] = 1\n",
        "        gby_col_truth_var = torch.from_numpy(truth_prob).to(device)\n",
        "        gby_col_prob = self.sigm(gby_score)\n",
        "        gby_bce_loss = -torch.mean( 3*(gby_col_truth_var * \\\n",
        "                torch.log(gby_col_prob+1e-10)) + \\\n",
        "                (1-gby_col_truth_var) * torch.log(1-gby_col_prob+1e-10) )\n",
        "        loss += gby_bce_loss\n",
        "        # Evaluate having\n",
        "        having_truth = [1 if len(x[13]) == 1 else 0 for x in truth_num]\n",
        "        having_truth_var = torch.from_numpy(np.array(having_truth)).to(device)\n",
        "        loss += self.CE(hv_score, having_truth_var)\n",
        "        # Evaluate having col\n",
        "        T = len(hv_col_score[0])\n",
        "        truth_prob = np.zeros((B, T), dtype=np.float32)\n",
        "        for b in range(B):\n",
        "            if len(truth_num[b][13]) > 0:\n",
        "                truth_prob[b][truth_num[b][13]] = 1\n",
        "        hv_col_truth_var = torch.from_numpy(truth_prob).to(device)\n",
        "        hv_col_prob = self.sigm(hv_col_score)\n",
        "        hv_col_bce_loss = -torch.mean( 3*(hv_col_truth_var * \\\n",
        "                torch.log(hv_col_prob+1e-10)) + \\\n",
        "                (1-hv_col_truth_var) * torch.log(1-hv_col_prob+1e-10) )\n",
        "        loss += hv_col_bce_loss\n",
        "        # Evaluate having agg\n",
        "        T = len(hv_agg_score[0])\n",
        "        truth_prob = np.zeros((B, T), dtype=np.float32)\n",
        "        for b in range(B):\n",
        "            if len(truth_num[b][12]) > 0:\n",
        "                truth_prob[b][truth_num[b][12]] = 1\n",
        "        hv_agg_truth_var = torch.from_numpy(truth_prob).to(device)\n",
        "        hv_agg_prob = self.sigm(hv_agg_truth_var)\n",
        "        hv_agg_bce_loss = -torch.mean( 3*(hv_agg_truth_var * \\\n",
        "                torch.log(hv_agg_prob+1e-10)) + \\\n",
        "                (1-hv_agg_truth_var) * torch.log(1-hv_agg_prob+1e-10) )\n",
        "        loss += hv_agg_bce_loss\n",
        "        # Evaluate having op\n",
        "        T = len(hv_op_score[0])\n",
        "        truth_prob = np.zeros((B, T), dtype=np.float32)\n",
        "        for b in range(B):\n",
        "            if len(truth_num[b][14]) > 0:\n",
        "                truth_prob[b][truth_num[b][14]] = 1\n",
        "        hv_op_truth_var = torch.from_numpy(truth_prob).to(device)\n",
        "        hv_op_prob = self.sigm(hv_op_truth_var)\n",
        "        hv_op_bce_loss = -torch.mean( 3*(hv_op_truth_var * \\\n",
        "                torch.log(hv_op_prob+1e-10)) + \\\n",
        "                (1-hv_op_truth_var) * torch.log(1-hv_op_prob+1e-10) )\n",
        "        loss += hv_op_bce_loss\n",
        "\n",
        "        \n",
        "        #TODO:IMPLEMENT YOUR CODE BELOW\n",
        "        # -----------loss for order_pred -------------- #\n",
        "        \n",
        "        # Evaluate the number of order by columns, use cross entropy loss\n",
        "        ody_num_truth = list(map(lambda x: x[10], truth_num))\n",
        "        ody_num_truth_var = torch.from_numpy(np.array(ody_num_truth)).to(device)\n",
        "        loss += self.CE(ody_num_score, ody_num_truth_var)\n",
        "        \n",
        "        # Evaluate the order by columns, use weighted negative log-likelihood bce loss\n",
        "        T = len(ody_col_score[0])\n",
        "        truth_prob = np.zeros((B, T), dtype=np.float32)\n",
        "        for b in range(B):\n",
        "            if len(truth_num[b][9]) > 0:\n",
        "                truth_prob[b][list(truth_num[b][9])] = 1\n",
        "        ody_col_truth_var = torch.from_numpy(truth_prob).to(device)\n",
        "        ody_col_prob = self.sigm(ody_col_score)\n",
        "        ody_col_bce_loss = -torch.mean( 3*(ody_col_truth_var * \\\n",
        "                torch.log(ody_col_prob+1e-10)) + \\\n",
        "                (1-ody_col_truth_var) * torch.log(1-ody_col_prob+1e-10) )\n",
        "        loss += ody_col_bce_loss\n",
        "        \n",
        "        # Evaluate order agg assume only one, use weighted negative log-likelihood bce loss\n",
        "        T = len(ody_agg_score[0])   # 6\n",
        "        truth_prob = np.zeros((B, T), dtype=np.float32)\n",
        "        for b in range(B):\n",
        "            if len(truth_num[b][9]) > 0:\n",
        "                truth_prob[b][list(truth_num[b][8])] = 1\n",
        "        ody_agg_truth_var = torch.from_numpy(truth_prob).to(device)\n",
        "        ody_agg_prob = self.sigm(ody_agg_score)\n",
        "        ody_agg_bce_loss = -torch.mean( 3*(ody_agg_truth_var * \\\n",
        "                torch.log(ody_agg_prob+1e-10)) + \\\n",
        "                (1-ody_agg_truth_var) * torch.log(1-ody_agg_prob+1e-10) )\n",
        "        loss += ody_agg_bce_loss \n",
        "\n",
        "        # Evaluate orderby parity, use cross entropy loss\n",
        "        ody_par_truth = list(map(lambda x: x[11], truth_num))\n",
        "        ody_par_truth_var = torch.from_numpy(np.array(ody_par_truth)).to(device)\n",
        "        loss += self.CE(ody_par_score, ody_par_truth_var)        \n",
        "        \n",
        "        return loss\n",
        "\n",
        "      \n",
        "    def check_acc(self, pred_queries, gt_queries):\n",
        "        '''\n",
        "        report accuracy of predictions for each submodule\n",
        "        \n",
        "        ---Parameters---\n",
        "        \n",
        "        pred_queries (list of dicts): each dict is the prediction result converted\n",
        "               by gen_query function based on predicted scores of each submodule\n",
        "        gt_queries (list of dicts): each dict the gold prediction result for each\n",
        "               data point\n",
        "        \n",
        "        ---Returns---\n",
        "        \n",
        "        err: error numbers for sel, cond, groupby, orderby, and all\n",
        "        \n",
        "        '''\n",
        "        B = len(gt_queries)\n",
        "\n",
        "        tot_err = 0.0\n",
        "        sel_err = agg_num_err = agg_op_err = sel_num_err = sel_col_err = 0.0\n",
        "        cond_err = cond_num_err = cond_col_err = cond_op_err = 0.0\n",
        "        gby_err = gby_num_err = gby_col_err = hv_err = hv_col_err = hv_agg_err = hv_op_err = 0.0\n",
        "        ody_err = ody_num_err = ody_col_err = ody_agg_err = ody_par_err = 0.0\n",
        "        \n",
        "        for b, (pred_qry, gt_qry) in enumerate(zip(pred_queries, gt_queries)):\n",
        "\n",
        "            good = True\n",
        "            tot_flag = True\n",
        "            sel_flag = True\n",
        "            cond_flag = True\n",
        "            gby_flag = True\n",
        "            ody_flag = True\n",
        "            \n",
        "            # accuracy for sel\n",
        "            sel_gt = gt_qry['sel']\n",
        "            sel_num_gt = len(set(sel_gt))\n",
        "            sel_pred = pred_qry['sel']\n",
        "            sel_num_pred = pred_qry['sel_num']\n",
        "            if sel_num_pred != sel_num_gt:\n",
        "                sel_num_err += 1\n",
        "                sel_flag = False\n",
        "            if sorted(set(sel_pred)) != sorted(set(sel_gt)):\n",
        "                sel_col_err += 1\n",
        "                sel_flag = False\n",
        "\n",
        "            agg_gt = gt_qry['agg']\n",
        "            curr_col = gt_qry['sel'][0]\n",
        "            curr_col_num_aggs = 0\n",
        "            gt_aggs_num = []\n",
        "            gt_sel_order = [curr_col]\n",
        "            for i, col in enumerate(gt_qry['sel']):\n",
        "                if col != curr_col:\n",
        "                    gt_sel_order.append(col)\n",
        "                    gt_aggs_num.append(curr_col_num_aggs)\n",
        "                    curr_col = col\n",
        "                    curr_col_num_aggs = 0\n",
        "                if agg_gt[i] != 0:\n",
        "                    curr_col_num_aggs += 1\n",
        "            gt_aggs_num.append(curr_col_num_aggs)\n",
        "\n",
        "            if pred_qry['agg_num'] != gt_aggs_num:\n",
        "                agg_num_err += 1\n",
        "                sel_flag = False\n",
        "\n",
        "            if sorted(pred_qry['agg']) != sorted(gt_qry['agg']): # naive\n",
        "                agg_op_err += 1\n",
        "                sel_flag = False\n",
        "\n",
        "            if not sel_flag:\n",
        "                sel_err += 1\n",
        "                good = False\n",
        "                \n",
        "            # accuracy for conds\n",
        "            cond_pred = pred_qry['conds']\n",
        "            cond_gt = gt_qry['cond']\n",
        "            flag = True\n",
        "            if len(cond_pred) != len(cond_gt):\n",
        "                flag = False\n",
        "                cond_num_err += 1\n",
        "                cond_flag = False\n",
        "            if flag and set(x[0] for x in cond_pred) != set(x[0] for x in cond_gt):\n",
        "                flag = False\n",
        "                cond_col_err += 1\n",
        "                cond_flag = False\n",
        "            for idx in range(len(cond_pred)):\n",
        "                if not flag:\n",
        "                    break\n",
        "                gt_idx = tuple(x[0] for x in cond_gt).index(cond_pred[idx][0])\n",
        "                if flag and cond_gt[gt_idx][1] != cond_pred[idx][1]:\n",
        "                    flag = False\n",
        "                    cond_op_err += 1\n",
        "                    cond_flag = False\n",
        "\n",
        "            if not cond_flag:\n",
        "                cond_err += 1\n",
        "                good = False\n",
        "                \n",
        "            # accuracy for group\n",
        "            gby_gt = gt_qry['group'][:-1]\n",
        "            gby_pred = pred_qry['group']\n",
        "            gby_num_pred = pred_qry['gby_num']\n",
        "            gby_num_gt = len(gby_gt)\n",
        "            if gby_num_pred != gby_num_gt:\n",
        "                gby_num_err += 1\n",
        "                gby_flag = False\n",
        "            if sorted(gby_pred) != sorted(gby_gt):\n",
        "                gby_col_err += 1\n",
        "                gby_flag = False\n",
        "            gt_gby_agg = gt_qry['group'][-1][0]\n",
        "            gt_gby_col = gt_qry['group'][-1][1]\n",
        "            gt_gby_op = gt_qry['group'][-1][2]\n",
        "            if gby_num_pred != 0 and len(gt_gby_col) != 0:\n",
        "                if pred_qry['hv'] != 1:\n",
        "                    hv_err += 1\n",
        "                    gby_flag = False\n",
        "                if pred_qry['hv_agg'] != gt_gby_agg[0]:\n",
        "                    hv_agg_err += 1\n",
        "                    gby_flag = False\n",
        "                if pred_qry['hv_col'] != gt_gby_col[0]:\n",
        "                    hv_col_err += 1\n",
        "                    gby_flag = False\n",
        "                if pred_qry['hv_op'] != gt_gby_op[0]:\n",
        "                    hv_op_err += 1\n",
        "                    gby_flag = False\n",
        "\n",
        "            if not gby_flag:\n",
        "                gby_err += 1\n",
        "                good = False\n",
        "\n",
        "            # accuracy for order\n",
        "            ody_gt_aggs = gt_qry['order'][0]\n",
        "            ody_gt_cols = gt_qry['order'][1]\n",
        "            ody_gt_par = gt_qry['order'][2]\n",
        "            ody_num_cols_pred = pred_qry['ody_num']\n",
        "            ody_cols_pred = pred_qry['order']\n",
        "            ody_aggs_pred = pred_qry['ody_agg']\n",
        "            ody_par_pred = pred_qry['parity']\n",
        "            \n",
        "            #TODO:IMPLEMENT YOUR CODE BELOW\n",
        "            # check if orderby column number prediction is right\n",
        "            if ody_num_cols_pred != len(ody_gt_cols):\n",
        "                ody_num_err += 1\n",
        "                ody_flag = False\n",
        "            \n",
        "            # if gold orderby col number is not 0, compute accuracy for \n",
        "            # 1. orderby column, 2. orderby agg, 3. orderby parity\n",
        "            if len(ody_gt_cols) > 0:\n",
        "                if ody_cols_pred != ody_gt_cols:\n",
        "                    ody_col_err += 1\n",
        "                    ody_flag = False\n",
        "                if ody_aggs_pred != ody_gt_aggs:\n",
        "                    ody_agg_err += 1\n",
        "                    ody_flag = False\n",
        "                if ody_par_pred != ody_gt_par:\n",
        "                    ody_par_err += 1\n",
        "                    ody_flag = False\n",
        "            \n",
        "            # update ody_err and 'good' flag\n",
        "            if not ody_flag:\n",
        "                ody_err += 1\n",
        "                good = False\n",
        "            \n",
        "            if not good:\n",
        "                tot_err += 1\n",
        "\n",
        "        return np.array((sel_err, cond_err, gby_err, ody_err)), tot_err\n",
        "\n",
        "\n",
        "    def gen_query(self, score, q, col, raw_q, raw_col, verbose=False):\n",
        "        '''\n",
        "        generate query prediction according to predicted scores of each submodule\n",
        "        '''\n",
        "        sel_score, cond_score, group_score, order_score = score\n",
        "\n",
        "        sel_num_score, sel_col_score, agg_num_score, agg_op_score = [x.data.cpu().numpy() if x is not None else None for x in sel_score]\n",
        "        cond_num_score, cond_col_score, cond_op_score = [x.data.cpu().numpy() if x is not None else None for x in cond_score]\n",
        "        gby_num_score, gby_score, hv_score, hv_col_score, hv_agg_score, hv_op_score = [x.data.cpu().numpy() if x is not None else None for x in group_score]\n",
        "        ody_num_score, ody_col_score, ody_agg_score, ody_par_score = [x.data.cpu().numpy() if x is not None else None for x in order_score]\n",
        "        \n",
        "        ret_queries = []\n",
        "        B = len(cond_num_score)\n",
        "        for b in range(B):\n",
        "            cur_query = {}\n",
        "             # ------------get sel predict\n",
        "            sel_num_cols = np.argmax(sel_num_score[b]) + 1\n",
        "            cur_query['sel_num'] = sel_num_cols\n",
        "            cur_query['sel'] = np.argsort(-sel_col_score[b])[:sel_num_cols]\n",
        "\n",
        "            agg_nums = []\n",
        "            agg_preds = []\n",
        "            for idx in range(sel_num_cols):\n",
        "                curr_num_aggs = np.argmax(agg_num_score[b][idx])\n",
        "                agg_nums.append(curr_num_aggs)\n",
        "                if curr_num_aggs == 0:\n",
        "                    curr_agg_ops = [0]\n",
        "                else:\n",
        "                    curr_agg_ops = [x for x in list(np.argsort(-agg_op_score[b][idx])) if x != 0][:curr_num_aggs]\n",
        "                agg_preds += curr_agg_ops\n",
        "            cur_query['agg_num'] = agg_nums\n",
        "            cur_query['agg'] = agg_preds\n",
        "            \n",
        "            #---------get cond predict\n",
        "            cur_query['conds'] = []\n",
        "            cond_num = np.argmax(cond_num_score[b])\n",
        "            max_idxes = np.argsort(-cond_col_score[b])[:cond_num]\n",
        "            for idx in range(cond_num):\n",
        "                cur_cond = []\n",
        "                cur_cond.append(max_idxes[idx])\n",
        "                cur_cond.append(np.argmax(cond_op_score[b][idx]))\n",
        "                cur_query['conds'].append(cur_cond)\n",
        "            \n",
        "            #----------get group by predict\n",
        "            gby_num_cols = np.argmax(gby_num_score[b])\n",
        "            cur_query['gby_num'] = gby_num_cols\n",
        "            cur_query['group'] = np.argsort(-gby_score[b])[:gby_num_cols]\n",
        "            cur_query['hv'] = np.argmax(hv_score[b])\n",
        "            if gby_num_cols != 0 and cur_query['hv'] != 0:\n",
        "                cur_query['hv_agg'] = np.argmax(hv_agg_score[b])\n",
        "                cur_query['hv_col'] = np.argmax(hv_col_score[b])\n",
        "                cur_query['hv_op'] = np.argmax(hv_op_score[b])\n",
        "            else:\n",
        "                cur_query['hv'] = 0\n",
        "                cur_query['hv_agg'] = 0\n",
        "                cur_query['hv_col'] = -1\n",
        "                cur_query['hv_op'] = -1\n",
        "                \n",
        "            # --------get order by\n",
        "            ody_num_cols = np.argmax(ody_num_score[b])\n",
        "            cur_query['ody_num'] = ody_num_cols\n",
        "            cur_query['order'] = np.argsort(-ody_col_score[b])[:ody_num_cols]\n",
        "            if ody_num_cols != 0:\n",
        "                cur_query['ody_agg'] = np.argmax(ody_agg_score[b])\n",
        "                cur_query['parity'] = np.argmax(ody_par_score[b])\n",
        "            else:\n",
        "                cur_query['ody_agg'] = 0\n",
        "                cur_query['parity'] = -1\n",
        "                \n",
        "            ret_queries.append(cur_query)\n",
        "\n",
        "        return ret_queries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXhjBpI2456J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6. Batching\n",
        "\n",
        "Generate batches of input samples."
      ]
    },
    {
      "metadata": {
        "id": "x7_COPkM46Hh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#READCODE! understand final inputs to the model\n",
        "def to_batch_seq(sql_data, idxes, start, end, ret_vis_data=False):\n",
        "    '''\n",
        "    get batch inputs for the model\n",
        "    \n",
        "    ---Parameters---\n",
        "    \n",
        "    sql_data (list of dicts): each dict in the list is a training data point \n",
        "            with question, database info, and target labels for each module \n",
        "            (agg/sel/SELECT, cond/WHERE, group/GROUP, order/ORDER) in SQLNet\n",
        "    idxes, start, end: indices for batching\n",
        "    \n",
        "    ---Returns---\n",
        "    too many... please check them by yourself\n",
        "    '''\n",
        "    q_seq = []\n",
        "    col_seq = []\n",
        "    col_num = []\n",
        "    ans_seq = []\n",
        "    query_seq = []\n",
        "    gt_cond_seq = []\n",
        "    vis_seq = []\n",
        "\n",
        "    col_org_seq = []\n",
        "\n",
        "    for i in range(start, end):\n",
        "        sql = sql_data[idxes[i]]\n",
        "        col_org_seq.append(sql['col_org'])\n",
        "        q_seq.append(sql['question_tok'])\n",
        "        column_names = sql[\"column_names\"]\n",
        "        col_num.append(len(column_names))\n",
        "        tab_cols = [col[1] for col in column_names]\n",
        "        col_seq.append([x.split(\" \") for x in tab_cols])\n",
        "        ans_seq.append((sql['agg'],     # sel agg # 0\n",
        "            sql['sel'],                 # sel col # 1\n",
        "            len(sql['cond']),           # cond # 2\n",
        "            tuple(x[0] for x in sql['cond']), # cond col 3\n",
        "            tuple(x[1] for x in sql['cond']), # cond op 4\n",
        "            len(set(sql['sel'])),       # number of unique select cols 5\n",
        "            sql['group'][:-1],          # group by columns 6\n",
        "            len(sql['group']) - 1,      # number of group by columns 7\n",
        "            sql['order'][0],            # order by aggregations 8\n",
        "            sql['order'][1],            # order by columns 9\n",
        "            len(sql['order'][1]),       # num order by columns 10\n",
        "            sql['order'][2],            # order by parity 11\n",
        "            sql['group'][-1][0],        # having agg 12\n",
        "            sql['group'][-1][1],        # having col 13\n",
        "            sql['group'][-1][2]         # having op 14\n",
        "            ))\n",
        "        #order: [[agg], [col], [dat]]\n",
        "        #group: [col1, col2, [agg, col, op]]\n",
        "        query_seq.append(sql['query_tok'])\n",
        "        gt_cond_seq.append([x for x in sql['cond']])\n",
        "        vis_seq.append((sql['question'], tab_cols, sql['query']))\n",
        "\n",
        "    if ret_vis_data:\n",
        "        return q_seq, col_seq, col_num, ans_seq, query_seq, gt_cond_seq, vis_seq, col_org_seq\n",
        "    else:\n",
        "        return q_seq, col_seq, col_num, ans_seq, query_seq, gt_cond_seq, col_org_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KOopsCs54tZ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 7. Model Traing and Testing\n",
        "\n",
        "Initialize SQLNet and optimizer below:"
      ]
    },
    {
      "metadata": {
        "id": "3nEhg_fv4tkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N_word=50 #set embedding dimension to 50\n",
        "#initialize SQLNet model, and move it to device (cuda or cpu)\n",
        "model = SQLNet(word_emb, N_word=N_word).to(device)\n",
        "learning_rate = 1e-3\n",
        "#initialize optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "II9vSgHCLnhm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the train and test procedures on one epoch of the data:"
      ]
    },
    {
      "metadata": {
        "id": "Nnx2IQLs5JTg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#READCODE!\n",
        "def epoch_train(model, optimizer, batch_size, sql_data):\n",
        "    '''\n",
        "    train model on one epoch of the data\n",
        "    '''\n",
        "    model.train() #set to training mode\n",
        "    perm=np.random.permutation(len(sql_data))\n",
        "    cum_loss = 0.0\n",
        "    st = 0\n",
        "    while st < len(sql_data): #until one epoch\n",
        "        ed = st+batch_size if st+batch_size < len(perm) else len(perm)\n",
        "        #batching data input\n",
        "        q_seq, col_seq, col_num, ans_seq, query_seq, gt_cond_seq, col_org_seq = \\\n",
        "                to_batch_seq(sql_data, perm, st, ed)\n",
        "        gt_sel_seq = [x[1] for x in ans_seq]\n",
        "        #1. call forward pass of SQLNet, 2.compute loss\n",
        "        score = model.forward(q_seq, col_seq, gt_cond=gt_cond_seq, gt_sel=gt_sel_seq)\n",
        "        #compute loss\n",
        "        loss = model.loss(score, ans_seq)\n",
        "        #add loss\n",
        "        cum_loss += loss.item()*(ed - st)\n",
        "        #recognize them?\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        st = ed\n",
        "\n",
        "    return cum_loss / len(sql_data)\n",
        "\n",
        "\n",
        "def epoch_acc(model, batch_size, sql_data):\n",
        "    '''\n",
        "    check accuracy of the model\n",
        "    '''\n",
        "    model.eval() # set to evaluation mode\n",
        "    perm = list(range(len(sql_data)))\n",
        "    st = 0\n",
        "    one_acc_num = 0.0\n",
        "    tot_acc_num = 0.0\n",
        "    while st < len(sql_data): #until one epoch\n",
        "        ed = st+batch_size if st+batch_size < len(perm) else len(perm)\n",
        "        #batching data input\n",
        "        q_seq, col_seq, col_num, ans_seq, query_seq, gt_cond_seq,\\\n",
        "         raw_data, col_org_seq = to_batch_seq(sql_data, perm, st, ed, ret_vis_data=True)\n",
        "        raw_q_seq = [x[0] for x in raw_data]\n",
        "        raw_col_seq = [x[1] for x in raw_data]\n",
        "        #batching gold query\n",
        "        query_gt, table_ids = to_batch_query(sql_data, perm, st, ed)\n",
        "        #forward pass\n",
        "        score = model.forward(q_seq, col_seq)\n",
        "        #generate predicted query\n",
        "        pred_queries = model.gen_query(score, q_seq, col_seq, raw_q_seq, raw_col_seq)\n",
        "        #check accuracy\n",
        "        one_err, tot_err = model.check_acc(pred_queries, query_gt)\n",
        "        #add error number\n",
        "        one_acc_num += (ed-st-one_err)\n",
        "        tot_acc_num += (ed-st-tot_err)\n",
        "\n",
        "        st = ed\n",
        "    return tot_acc_num / len(sql_data), one_acc_num / len(sql_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cHLTO8k6L9TE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we can run the train and test processes, print the results, and save the models:\n",
        "\n",
        "**Note**: We already include enough info in this assignment. So the final evaluation step that computes the set matching accuracy of complete predicted and gold SQL queries is not covered here. You can refer to [the Spider Github page](https://github.com/taoyds/spider/tree/master/evaluation_examples) for more details if you are interested."
      ]
    },
    {
      "metadata": {
        "id": "Ik_K44P75Kcg",
        "colab_type": "code",
        "outputId": "41936a73-135d-4a97-e551-43f22806f798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4100
        }
      },
      "cell_type": "code",
      "source": [
        "#READCODE!\n",
        "BATCH_SIZE=64\n",
        "#initial accuracy\n",
        "init_acc = epoch_acc(model, BATCH_SIZE, val_sql_data)\n",
        "best_sel_acc = init_acc[1][0]\n",
        "best_cond_acc = init_acc[1][1]\n",
        "best_group_acc = init_acc[1][2]\n",
        "best_order_acc = init_acc[1][3]\n",
        "best_tot_acc = 0.0\n",
        "\n",
        "for i in range(30):\n",
        "    print('Epoch %d @ %s'%(i+1, datetime.datetime.now()))\n",
        "    print(' Loss = %s'%epoch_train(model, optimizer, BATCH_SIZE, train_sql_data))\n",
        "    train_tot_acc, train_bkd_acc = epoch_acc(model, BATCH_SIZE, train_sql_data)\n",
        "    print(' Train acc_qm: %s' % train_tot_acc)\n",
        "    print(' Breakdown results: sel: %s, cond: %s, group: %s, order: %s'\\\n",
        "        % (train_bkd_acc[0], train_bkd_acc[1], train_bkd_acc[2], train_bkd_acc[3]))\n",
        "\n",
        "    val_tot_acc, val_bkd_acc = epoch_acc(model, BATCH_SIZE, val_sql_data) #for detailed error analysis, pass True to error_print\n",
        "    print(' Dev acc_qm: %s' % val_tot_acc)\n",
        "    print(' Breakdown results: sel: %s, cond: %s, group: %s, order: %s'\\\n",
        "        % (val_bkd_acc[0], val_bkd_acc[1], val_bkd_acc[2], val_bkd_acc[3]))\n",
        "\n",
        "    #save models\n",
        "    if val_bkd_acc[0] > best_sel_acc:\n",
        "        best_sel_acc = val_bkd_acc[0]\n",
        "        print(\"Saving sel model...\")\n",
        "        torch.save(model.sel_pred.state_dict(), os.path.join(SAVED_MODEL_DIR, \"sel_models.dump\"))\n",
        "    if val_bkd_acc[1] > best_cond_acc:\n",
        "        best_cond_acc = val_bkd_acc[1]\n",
        "        print(\"Saving cond model...\")\n",
        "        torch.save(model.cond_pred.state_dict(), os.path.join(SAVED_MODEL_DIR, \"cond_models.dump\"))\n",
        "    if val_bkd_acc[2] > best_group_acc:\n",
        "        best_group_acc = val_bkd_acc[2]\n",
        "        print(\"Saving group model...\") \n",
        "        torch.save(model.group_pred.state_dict(), os.path.join(SAVED_MODEL_DIR, \"group_models.dump\"))\n",
        "    if val_bkd_acc[3] > best_order_acc:\n",
        "        best_order_acc = val_bkd_acc[3]\n",
        "        print(\"Saving order model...\")\n",
        "        torch.save(model.order_pred.state_dict(), os.path.join(SAVED_MODEL_DIR, \"order_models.dump\"))\n",
        "    if val_tot_acc > best_tot_acc:\n",
        "        best_tot_acc = val_tot_acc\n",
        "\n",
        "\n",
        "    print(' Best val sel = %s, cond = %s, group = %s, order = %s, tot = %s'%(best_sel_acc, best_cond_acc, best_group_acc, best_order_acc, best_tot_acc))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 @ 2019-04-09 21:49:55.605207\n",
            " Loss = 7.2532005255562915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:410: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Train acc_qm: 0.011285714285714286\n",
            " Breakdown results: sel: 0.08714285714285715, cond: 0.36842857142857144, group: 0.7508571428571429, order: 0.7304285714285714\n",
            " Dev acc_qm: 0.008704061895551257\n",
            " Breakdown results: sel: 0.10638297872340426, cond: 0.3413926499032882, group: 0.7350096711798839, order: 0.7079303675048356\n",
            "Saving sel model...\n",
            "Saving cond model...\n",
            "Saving group model...\n",
            "Saving order model...\n",
            " Best val sel = 0.10638297872340426, cond = 0.3413926499032882, group = 0.7350096711798839, order = 0.7079303675048356, tot = 0.008704061895551257\n",
            "Epoch 2 @ 2019-04-09 21:52:20.139362\n",
            " Loss = 5.317157589503697\n",
            " Train acc_qm: 0.008285714285714285\n",
            " Breakdown results: sel: 0.09471428571428571, cond: 0.43257142857142855, group: 0.6728571428571428, order: 0.7961428571428572\n",
            " Dev acc_qm: 0.005802707930367505\n",
            " Breakdown results: sel: 0.11798839458413926, cond: 0.42940038684719534, group: 0.648936170212766, order: 0.7775628626692457\n",
            "Saving sel model...\n",
            "Saving cond model...\n",
            "Saving order model...\n",
            " Best val sel = 0.11798839458413926, cond = 0.42940038684719534, group = 0.7350096711798839, order = 0.7775628626692457, tot = 0.008704061895551257\n",
            "Epoch 3 @ 2019-04-09 21:54:46.204914\n",
            " Loss = 4.5341595279148645\n",
            " Train acc_qm: 0.03371428571428572\n",
            " Breakdown results: sel: 0.14985714285714286, cond: 0.449, group: 0.7091428571428572, order: 0.8104285714285714\n",
            " Dev acc_qm: 0.04448742746615087\n",
            " Breakdown results: sel: 0.21083172147001933, cond: 0.4574468085106383, group: 0.688588007736944, order: 0.8085106382978723\n",
            "Saving sel model...\n",
            "Saving cond model...\n",
            "Saving order model...\n",
            " Best val sel = 0.21083172147001933, cond = 0.4574468085106383, group = 0.7350096711798839, order = 0.8085106382978723, tot = 0.04448742746615087\n",
            "Epoch 4 @ 2019-04-09 21:57:10.310820\n",
            " Loss = 4.137586430413382\n",
            " Train acc_qm: 0.04371428571428571\n",
            " Breakdown results: sel: 0.17357142857142857, cond: 0.4288571428571429, group: 0.7342857142857143, order: 0.8271428571428572\n",
            " Dev acc_qm: 0.05125725338491296\n",
            " Breakdown results: sel: 0.2437137330754352, cond: 0.4284332688588008, group: 0.7156673114119922, order: 0.8239845261121856\n",
            "Saving sel model...\n",
            "Saving order model...\n",
            " Best val sel = 0.2437137330754352, cond = 0.4574468085106383, group = 0.7350096711798839, order = 0.8239845261121856, tot = 0.05125725338491296\n",
            "Epoch 5 @ 2019-04-09 21:59:33.338166\n",
            " Loss = 3.820557077407837\n",
            " Train acc_qm: 0.046142857142857145\n",
            " Breakdown results: sel: 0.17942857142857144, cond: 0.48242857142857143, group: 0.718, order: 0.8265714285714286\n",
            " Dev acc_qm: 0.059961315280464215\n",
            " Breakdown results: sel: 0.2562862669245648, cond: 0.48452611218568664, group: 0.6827852998065764, order: 0.8249516441005803\n",
            "Saving sel model...\n",
            "Saving cond model...\n",
            "Saving order model...\n",
            " Best val sel = 0.2562862669245648, cond = 0.48452611218568664, group = 0.7350096711798839, order = 0.8249516441005803, tot = 0.059961315280464215\n",
            "Epoch 6 @ 2019-04-09 22:01:57.150905\n",
            " Loss = 3.6295712285723005\n",
            " Train acc_qm: 0.04757142857142857\n",
            " Breakdown results: sel: 0.187, cond: 0.4888571428571429, group: 0.7367142857142858, order: 0.8335714285714285\n",
            " Dev acc_qm: 0.06576402321083172\n",
            " Breakdown results: sel: 0.2640232108317215, cond: 0.4864603481624758, group: 0.7040618955512572, order: 0.8326885880077369\n",
            "Saving sel model...\n",
            "Saving cond model...\n",
            "Saving order model...\n",
            " Best val sel = 0.2640232108317215, cond = 0.4864603481624758, group = 0.7350096711798839, order = 0.8326885880077369, tot = 0.06576402321083172\n",
            "Epoch 7 @ 2019-04-09 22:04:22.326935\n",
            " Loss = 3.4739819483075824\n",
            " Train acc_qm: 0.052\n",
            " Breakdown results: sel: 0.18271428571428572, cond: 0.5117142857142857, group: 0.745, order: 0.8262857142857143\n",
            " Dev acc_qm: 0.06673114119922631\n",
            " Breakdown results: sel: 0.23984526112185686, cond: 0.5203094777562862, group: 0.7195357833655706, order: 0.804642166344294\n",
            "Saving cond model...\n",
            " Best val sel = 0.2640232108317215, cond = 0.5203094777562862, group = 0.7350096711798839, order = 0.8326885880077369, tot = 0.06673114119922631\n",
            "Epoch 8 @ 2019-04-09 22:06:46.254440\n",
            " Loss = 3.337534571783883\n",
            " Train acc_qm: 0.05442857142857143\n",
            " Breakdown results: sel: 0.18985714285714286, cond: 0.5378571428571428, group: 0.732, order: 0.8208571428571428\n",
            " Dev acc_qm: 0.0735009671179884\n",
            " Breakdown results: sel: 0.2620889748549323, cond: 0.558027079303675, group: 0.6992263056092843, order: 0.8017408123791102\n",
            "Saving cond model...\n",
            " Best val sel = 0.2640232108317215, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8326885880077369, tot = 0.0735009671179884\n",
            "Epoch 9 @ 2019-04-09 22:09:10.576580\n",
            " Loss = 3.1860861571175714\n",
            " Train acc_qm: 0.054285714285714284\n",
            " Breakdown results: sel: 0.19614285714285715, cond: 0.5258571428571429, group: 0.714, order: 0.8431428571428572\n",
            " Dev acc_qm: 0.07640232108317214\n",
            " Breakdown results: sel: 0.2688588007736944, cond: 0.5338491295938105, group: 0.6663442940038685, order: 0.8317214700193424\n",
            "Saving sel model...\n",
            " Best val sel = 0.2688588007736944, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8326885880077369, tot = 0.07640232108317214\n",
            "Epoch 10 @ 2019-04-09 22:11:33.805057\n",
            " Loss = 3.096678188596453\n",
            " Train acc_qm: 0.053285714285714283\n",
            " Breakdown results: sel: 0.19542857142857142, cond: 0.5055714285714286, group: 0.7068571428571429, order: 0.8432857142857143\n",
            " Dev acc_qm: 0.07059961315280464\n",
            " Breakdown results: sel: 0.2553191489361702, cond: 0.5029013539651838, group: 0.6499032882011605, order: 0.8317214700193424\n",
            " Best val sel = 0.2688588007736944, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8326885880077369, tot = 0.07640232108317214\n",
            "Epoch 11 @ 2019-04-09 22:13:47.634882\n",
            " Loss = 2.9964893981388636\n",
            " Train acc_qm: 0.05542857142857143\n",
            " Breakdown results: sel: 0.19242857142857142, cond: 0.5285714285714286, group: 0.7362857142857143, order: 0.8494285714285714\n",
            " Dev acc_qm: 0.08220502901353965\n",
            " Breakdown results: sel: 0.2572533849129594, cond: 0.5290135396518375, group: 0.6818181818181818, order: 0.8413926499032882\n",
            "Saving order model...\n",
            " Best val sel = 0.2688588007736944, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8413926499032882, tot = 0.08220502901353965\n",
            "Epoch 12 @ 2019-04-09 22:16:12.968594\n",
            " Loss = 2.8900946494511195\n",
            " Train acc_qm: 0.054714285714285715\n",
            " Breakdown results: sel: 0.18528571428571428, cond: 0.49057142857142855, group: 0.747, order: 0.8537142857142858\n",
            " Dev acc_qm: 0.06479690522243714\n",
            " Breakdown results: sel: 0.2437137330754352, cond: 0.4622823984526112, group: 0.7117988394584139, order: 0.8520309477756286\n",
            "Saving order model...\n",
            " Best val sel = 0.2688588007736944, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8520309477756286, tot = 0.08220502901353965\n",
            "Epoch 13 @ 2019-04-09 22:18:38.982734\n",
            " Loss = 2.8795376126425607\n",
            " Train acc_qm: 0.05728571428571429\n",
            " Breakdown results: sel: 0.19614285714285715, cond: 0.5402857142857143, group: 0.7262857142857143, order: 0.8541428571428571\n",
            " Dev acc_qm: 0.08220502901353965\n",
            " Breakdown results: sel: 0.2553191489361702, cond: 0.5367504835589942, group: 0.6721470019342359, order: 0.8404255319148937\n",
            " Best val sel = 0.2688588007736944, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8520309477756286, tot = 0.08220502901353965\n",
            "Epoch 14 @ 2019-04-09 22:21:04.609479\n",
            " Loss = 2.7134429234095983\n",
            " Train acc_qm: 0.05785714285714286\n",
            " Breakdown results: sel: 0.19842857142857143, cond: 0.5164285714285715, group: 0.732, order: 0.8631428571428571\n",
            " Dev acc_qm: 0.07930367504835589\n",
            " Breakdown results: sel: 0.2562862669245648, cond: 0.5, group: 0.6914893617021277, order: 0.8500967117988395\n",
            " Best val sel = 0.2688588007736944, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8520309477756286, tot = 0.08220502901353965\n",
            "Epoch 15 @ 2019-04-09 22:23:29.951580\n",
            " Loss = 2.6879943190983364\n",
            " Train acc_qm: 0.05914285714285714\n",
            " Breakdown results: sel: 0.2057142857142857, cond: 0.5488571428571428, group: 0.6938571428571428, order: 0.8628571428571429\n",
            " Dev acc_qm: 0.0725338491295938\n",
            " Breakdown results: sel: 0.25918762088974856, cond: 0.5444874274661509, group: 0.6237911025145068, order: 0.8462282398452611\n",
            " Best val sel = 0.2688588007736944, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8520309477756286, tot = 0.08220502901353965\n",
            "Epoch 16 @ 2019-04-09 22:25:53.353929\n",
            " Loss = 2.6244166870117187\n",
            " Train acc_qm: 0.06914285714285714\n",
            " Breakdown results: sel: 0.20885714285714285, cond: 0.5472857142857143, group: 0.7641428571428571, order: 0.8711428571428571\n",
            " Dev acc_qm: 0.08897485493230174\n",
            " Breakdown results: sel: 0.25822050290135395, cond: 0.5309477756286267, group: 0.7292069632495164, order: 0.8500967117988395\n",
            " Best val sel = 0.2688588007736944, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8520309477756286, tot = 0.08897485493230174\n",
            "Epoch 17 @ 2019-04-09 22:28:13.217033\n",
            " Loss = 2.5423857007707866\n",
            " Train acc_qm: 0.06857142857142857\n",
            " Breakdown results: sel: 0.21014285714285713, cond: 0.5442857142857143, group: 0.753, order: 0.872\n",
            " Dev acc_qm: 0.08897485493230174\n",
            " Breakdown results: sel: 0.2727272727272727, cond: 0.5193423597678917, group: 0.7021276595744681, order: 0.8539651837524178\n",
            "Saving sel model...\n",
            "Saving order model...\n",
            " Best val sel = 0.2727272727272727, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8539651837524178, tot = 0.08897485493230174\n",
            "Epoch 18 @ 2019-04-09 22:30:38.011465\n",
            " Loss = 2.4935222832815986\n",
            " Train acc_qm: 0.06714285714285714\n",
            " Breakdown results: sel: 0.21042857142857144, cond: 0.5214285714285715, group: 0.7531428571428571, order: 0.8728571428571429\n",
            " Dev acc_qm: 0.08413926499032882\n",
            " Breakdown results: sel: 0.2678916827852998, cond: 0.4864603481624758, group: 0.7059961315280464, order: 0.8346228239845261\n",
            " Best val sel = 0.2727272727272727, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8539651837524178, tot = 0.08897485493230174\n",
            "Epoch 19 @ 2019-04-09 22:33:02.756890\n",
            " Loss = 2.452490427562169\n",
            " Train acc_qm: 0.07228571428571429\n",
            " Breakdown results: sel: 0.213, cond: 0.5542857142857143, group: 0.7631428571428571, order: 0.8781428571428571\n",
            " Dev acc_qm: 0.08994197292069632\n",
            " Breakdown results: sel: 0.269825918762089, cond: 0.5386847195357833, group: 0.718568665377176, order: 0.8520309477756286\n",
            " Best val sel = 0.2727272727272727, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8539651837524178, tot = 0.08994197292069632\n",
            "Epoch 20 @ 2019-04-09 22:35:25.063858\n",
            " Loss = 2.4088190081460135\n",
            " Train acc_qm: 0.06942857142857142\n",
            " Breakdown results: sel: 0.2092857142857143, cond: 0.5515714285714286, group: 0.7652857142857142, order: 0.8778571428571429\n",
            " Dev acc_qm: 0.09090909090909091\n",
            " Breakdown results: sel: 0.2572533849129594, cond: 0.5386847195357833, group: 0.7137330754352031, order: 0.8491295938104448\n",
            " Best val sel = 0.2727272727272727, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8539651837524178, tot = 0.09090909090909091\n",
            "Epoch 21 @ 2019-04-09 22:37:48.767081\n",
            " Loss = 2.3290985785893032\n",
            " Train acc_qm: 0.07271428571428572\n",
            " Breakdown results: sel: 0.219, cond: 0.5395714285714286, group: 0.7602857142857142, order: 0.8807142857142857\n",
            " Dev acc_qm: 0.0851063829787234\n",
            " Breakdown results: sel: 0.2601547388781431, cond: 0.4912959381044487, group: 0.7040618955512572, order: 0.8558994197292069\n",
            "Saving order model...\n",
            " Best val sel = 0.2727272727272727, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8558994197292069, tot = 0.09090909090909091\n",
            "Epoch 22 @ 2019-04-09 22:40:13.760904\n",
            " Loss = 2.289359140941075\n",
            " Train acc_qm: 0.07642857142857143\n",
            " Breakdown results: sel: 0.21771428571428572, cond: 0.5557142857142857, group: 0.7567142857142857, order: 0.8825714285714286\n",
            " Dev acc_qm: 0.08800773694390715\n",
            " Breakdown results: sel: 0.2572533849129594, cond: 0.5164410058027079, group: 0.6943907156673114, order: 0.8481624758220503\n",
            " Best val sel = 0.2727272727272727, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8558994197292069, tot = 0.09090909090909091\n",
            "Epoch 23 @ 2019-04-09 22:42:37.212805\n",
            " Loss = 2.309070537022182\n",
            " Train acc_qm: 0.07685714285714286\n",
            " Breakdown results: sel: 0.21971428571428572, cond: 0.5641428571428572, group: 0.752, order: 0.8804285714285714\n",
            " Dev acc_qm: 0.08994197292069632\n",
            " Breakdown results: sel: 0.26595744680851063, cond: 0.5377176015473888, group: 0.6876208897485493, order: 0.8549323017408124\n",
            " Best val sel = 0.2727272727272727, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8558994197292069, tot = 0.09090909090909091\n",
            "Epoch 24 @ 2019-04-09 22:45:00.470158\n",
            " Loss = 2.25143064226423\n",
            " Train acc_qm: 0.08028571428571428\n",
            " Breakdown results: sel: 0.21771428571428572, cond: 0.5625714285714286, group: 0.7695714285714286, order: 0.8864285714285715\n",
            " Dev acc_qm: 0.08317214700193423\n",
            " Breakdown results: sel: 0.26595744680851063, cond: 0.534816247582205, group: 0.7340425531914894, order: 0.8491295938104448\n",
            " Best val sel = 0.2727272727272727, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8558994197292069, tot = 0.09090909090909091\n",
            "Epoch 25 @ 2019-04-09 22:47:22.361226\n",
            " Loss = 2.189972674506051\n",
            " Train acc_qm: 0.08242857142857143\n",
            " Breakdown results: sel: 0.22457142857142856, cond: 0.554, group: 0.7672857142857142, order: 0.8864285714285715\n",
            " Dev acc_qm: 0.0851063829787234\n",
            " Breakdown results: sel: 0.2620889748549323, cond: 0.511605415860735, group: 0.718568665377176, order: 0.8578336557059961\n",
            "Saving order model...\n",
            " Best val sel = 0.2727272727272727, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8578336557059961, tot = 0.09090909090909091\n",
            "Epoch 26 @ 2019-04-09 22:49:45.890512\n",
            " Loss = 2.177378126961844\n",
            " Train acc_qm: 0.08228571428571428\n",
            " Breakdown results: sel: 0.22685714285714287, cond: 0.5718571428571428, group: 0.7661428571428571, order: 0.8851428571428571\n",
            " Dev acc_qm: 0.08897485493230174\n",
            " Breakdown results: sel: 0.2640232108317215, cond: 0.5570599613152805, group: 0.7108317214700194, order: 0.8404255319148937\n",
            " Best val sel = 0.2727272727272727, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8578336557059961, tot = 0.09090909090909091\n",
            "Epoch 27 @ 2019-04-09 22:52:01.049121\n",
            " Loss = 2.1684843087877548\n",
            " Train acc_qm: 0.08471428571428571\n",
            " Breakdown results: sel: 0.22757142857142856, cond: 0.5762857142857143, group: 0.7692857142857142, order: 0.8884285714285715\n",
            " Dev acc_qm: 0.09284332688588008\n",
            " Breakdown results: sel: 0.2804642166344294, cond: 0.5377176015473888, group: 0.7321083172147002, order: 0.839458413926499\n",
            "Saving sel model...\n",
            " Best val sel = 0.2804642166344294, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8578336557059961, tot = 0.09284332688588008\n",
            "Epoch 28 @ 2019-04-09 22:54:23.080779\n",
            " Loss = 2.1235083359309606\n",
            " Train acc_qm: 0.086\n",
            " Breakdown results: sel: 0.22528571428571428, cond: 0.5701428571428572, group: 0.7725714285714286, order: 0.8908571428571429\n",
            " Dev acc_qm: 0.09187620889748549\n",
            " Breakdown results: sel: 0.27079303675048355, cond: 0.5261121856866537, group: 0.7263056092843327, order: 0.8568665377176016\n",
            " Best val sel = 0.2804642166344294, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8578336557059961, tot = 0.09284332688588008\n",
            "Epoch 29 @ 2019-04-09 22:56:46.882927\n",
            " Loss = 2.088980818203517\n",
            " Train acc_qm: 0.08614285714285715\n",
            " Breakdown results: sel: 0.22785714285714287, cond: 0.5794285714285714, group: 0.7674285714285715, order: 0.8898571428571429\n",
            " Dev acc_qm: 0.09381044487427466\n",
            " Breakdown results: sel: 0.27176015473887816, cond: 0.5483558994197292, group: 0.7040618955512572, order: 0.8529980657640233\n",
            " Best val sel = 0.2804642166344294, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8578336557059961, tot = 0.09381044487427466\n",
            "Epoch 30 @ 2019-04-09 22:59:10.205667\n",
            " Loss = 2.0430272545133317\n",
            " Train acc_qm: 0.086\n",
            " Breakdown results: sel: 0.2317142857142857, cond: 0.5784285714285714, group: 0.7725714285714286, order: 0.888\n",
            " Dev acc_qm: 0.08800773694390715\n",
            " Breakdown results: sel: 0.27176015473887816, cond: 0.5483558994197292, group: 0.723404255319149, order: 0.8462282398452611\n",
            " Best val sel = 0.2804642166344294, cond = 0.558027079303675, group = 0.7350096711798839, order = 0.8578336557059961, tot = 0.09381044487427466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JKIhGkd0Zbnb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TODO**: Now you implemented one module in SQLNet. List two limitations of SQLNet model and two possible ideas for the Spider task. - ___10 points___ <br/>\n",
        "**YOUR ANSWER:** \n",
        "\n",
        "Limitations of SQLNet:\n",
        "1. SQLNet doesn't handle scenarios where nested SQL queries are needed. \n",
        "2. The ORDER BY module considers only 0 or 1 column, but in reality multiple columns can be included in the ORDER BY clause. \n",
        "\n",
        "Possible ideas for the Spider task:\n",
        "1. Build a diaglog interface that allows the system to interact with human to generate SQL queries that best suit human needs.\n",
        "2. Since queries are marked with difficulties, Spider can be used to compare the performance discrepancy of a particular model on queries of different difficulties. Or similarly, by comparing different models, Spider can be used to suggest the particular set of features that results in good/bad performance on queries of a specific level of difficulty."
      ]
    },
    {
      "metadata": {
        "id": "a-ocgn6u5Pqt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assignment Part 3: SQLNet with Torchtext - 35 Points\n",
        "\n",
        "\n",
        "In the last section, we did data preprocessing, numericalize and embedding, data loading and batching all by ourselves. These steps could be painful and tedious for many NLP tasks. You don't want to restart and write new code for a similar task. It's also very easy to make some small bugs and ruin all your work.\n",
        "\n",
        "In this part, let's try Torchtext to standardize the text-processing steps. For simplicity, we only demonstrate this transformation on the condition module.\n",
        "\n",
        "#### 1. Data Prepropessing"
      ]
    },
    {
      "metadata": {
        "id": "_8r9OPYgZFdX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#READCODE! Only go over the condition part, SCAN others\n",
        "def process_one(sql, tables):\n",
        "    '''\n",
        "    modified process function for torchtext, basically combine old process \n",
        "    and to_batch_seq functions\n",
        "    '''\n",
        "    sql_one = {}\n",
        "    \n",
        "    # add query metadata\n",
        "    sql_one['question'] = sql['question']\n",
        "    sql_one['question_tok'] = sql['question_toks']\n",
        "    sql_one['query'] = sql['query']\n",
        "    sql_one['query_tok'] = sql['query_toks']\n",
        "    sql_one['table_id'] = sql['db_id']\n",
        "    table = tables[sql['db_id']]\n",
        "    sql_one['col_org'] = table['column_names_original']\n",
        "    sql_one['table_org'] = table['table_names_original']\n",
        "    sql_one['foreign_keys'] = table['foreign_keys']\n",
        "    column_names = [x.split(\" \") for x in [col[1] for col in table['column_names']]]\n",
        "    sql_one['col_len'] = [len(x) for x in column_names]\n",
        "    sql_one['col_seq'] = [str(x) for col in column_names for x in col]\n",
        "    \n",
        "\n",
        "    # process agg/sel\n",
        "    sql_one['agg'] = []\n",
        "    sql_one['sel'] = []\n",
        "    gt_sel = sql['sql']['select'][1]\n",
        "    if len(gt_sel) > 3:\n",
        "        gt_sel = gt_sel[:3]\n",
        "    for tup in gt_sel:\n",
        "        sql_one['agg'].append(tup[0])\n",
        "        sql_one['sel'].append(tup[1][1][1]) #GOLD for sel and agg\n",
        "    sql_one[\"sel_num\"] = len(set(sql_one['sel']))\n",
        "\n",
        "    # process where conditions and conjuctions\n",
        "    sql_one['cond'] = []\n",
        "    gt_cond = sql['sql']['where']\n",
        "    if len(gt_cond) > 0:\n",
        "        conds = [gt_cond[x] for x in range(len(gt_cond)) if x % 2 == 0]\n",
        "        for cond in conds:\n",
        "            curr_cond = []\n",
        "            curr_cond.append(cond[2][1][1])\n",
        "            curr_cond.append(cond[1])\n",
        "            if cond[4] is not None:\n",
        "                curr_cond.append([cond[3], cond[4]])\n",
        "            else:\n",
        "                curr_cond.append(cond[3])\n",
        "            sql_one['cond'].append(curr_cond) #GOLD for COND [[col, op],[]]\n",
        "\n",
        "    sql_one['cond_num'] = len(sql_one['cond'])\n",
        "    sql_one['cond_col'] = [x[0] for x in sql_one['cond']]\n",
        "    sql_one['cond_op'] = [x[1] for x in sql_one['cond']]\n",
        "    sql_one['gt_cond'] = [x for x in sql_one['cond']]\n",
        "    sql_one['conj'] = [gt_cond[x] for x in range(len(gt_cond)) if x % 2 == 1]\n",
        "\n",
        "    # process group by / having\n",
        "    sql_one['group'] = [x[1] for x in sql['sql']['groupBy']] #assume only one groupby\n",
        "    having_cond = []\n",
        "    if len(sql['sql']['having']) > 0:\n",
        "        gt_having = sql['sql']['having'][0] # currently only do first having condition\n",
        "        having_cond.append([gt_having[2][1][0]]) # aggregator\n",
        "        having_cond.append([gt_having[2][1][1]]) # column\n",
        "        having_cond.append([gt_having[1]]) # operator\n",
        "        if gt_having[4] is not None:\n",
        "            having_cond.append([gt_having[3], gt_having[4]])\n",
        "        else:\n",
        "            having_cond.append(gt_having[3])\n",
        "    else:\n",
        "        having_cond = [[], [], []]\n",
        "    sql_one['group'].append(having_cond) #GOLD for GROUP [[col1, col2, [agg, col, op]], [col, []]]\n",
        "    sql_one[\"group_col\"] = sql_one['group'][:-1]\n",
        "    sql_one[\"group_num\"] = len(sql_one['group']) - 1\n",
        "    sql_one[\"having_agg\"] = sql_one['group'][-1][0]\n",
        "    sql_one[\"having_col\"] = sql_one['group'][-1][1]\n",
        "    sql_one[\"having_op\"] = sql_one['group'][-1][2]\n",
        "\n",
        "    # process order by / limit\n",
        "    order_aggs = []\n",
        "    order_cols = []\n",
        "    sql_one['order'] = []\n",
        "    order_par = 4\n",
        "    gt_order = sql['sql']['orderBy']\n",
        "    limit = sql['sql']['limit']\n",
        "    if len(gt_order) > 0:\n",
        "        order_aggs = [x[1][0] for x in gt_order[1][:1]] # limit to 1 order by\n",
        "        order_cols = [x[1][1] for x in gt_order[1][:1]]\n",
        "        if limit != None:\n",
        "            if gt_order[0] == 'asc':\n",
        "                order_par = 0\n",
        "            else:\n",
        "                order_par = 1\n",
        "        else:\n",
        "            if gt_order[0] == 'asc':\n",
        "                order_par = 2\n",
        "            else:\n",
        "                order_par = 3\n",
        "\n",
        "    sql_one['order'] = [order_aggs, order_cols, order_par] #GOLD for ORDER [[[agg], [col], [dat]], []]\n",
        "    sql_one[\"order_agg\"] = sql_one['order'][0]\n",
        "    sql_one[\"order_col\"] = sql_one['order'][1]\n",
        "    sql_one[\"order_num\"] = len(sql_one['order'][1])\n",
        "    sql_one[\"order_parity\"] = sql_one['order'][2]\n",
        "\n",
        "    # process intersect/except/union\n",
        "    sql_one['special'] = 0\n",
        "    if sql['sql']['intersect'] is not None:\n",
        "        sql_one['special'] = 1\n",
        "    elif sql['sql']['except'] is not None:\n",
        "        sql_one['special'] = 2\n",
        "    elif sql['sql']['union'] is not None:\n",
        "        sql_one['special'] = 3\n",
        "        \n",
        "    return sql_one\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I65juHXKeFXy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process(sql_path, table_data):\n",
        "    '''\n",
        "    process all input data\n",
        "    '''\n",
        "    with open(sql_path) as inf:\n",
        "        sql_data = json.load(inf)\n",
        "        \n",
        "    tables = {}\n",
        "    for i in range(len(table_data)):\n",
        "        table = table_data[i]\n",
        "        db_name = table['db_id']\n",
        "        tables[db_name] = table\n",
        "\n",
        "    output_sql = []\n",
        "    for i in range(len(sql_data)):\n",
        "        sql = sql_data[i]\n",
        "        sql_one = process_one(sql, tables)\n",
        "        output_sql.append(sql_one)\n",
        "\n",
        "    return output_sql"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d29HFf2D6bmG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2. Torchtext Fields and Examples\n",
        "\n",
        "Torchtext `Field`s  are used to define how you want the data preprocessed. The `Example` object bundles the attributes of a single data point together.\n",
        "\n",
        "**Note**: Please follow [toturial 1](http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/), [toturial 2](http://anie.me/On-Torchtext/) or the official [doc](https://torchtext.readthedocs.io/en/latest/data.html#) and [code](https://github.com/pytorch/text) to understand how Torchtext works."
      ]
    },
    {
      "metadata": {
        "id": "BB_RrPnceSta",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#READCODE!\n",
        "def get_fields():\n",
        "    '''\n",
        "    fields (inputs: question_tok, col_seq, col_len, and targets: cond_col, cond_op) for cond module\n",
        "    '''\n",
        "    fields = {}\n",
        "    fields[\"question_tok\"] = torchtext.data.Field(include_lengths=True, lower=True, batch_first=True)\n",
        "    fields[\"col_seq\"] = torchtext.data.Field(include_lengths=True, lower=True, batch_first=True)\n",
        "    fields[\"col_len\"] = torchtext.data.Field(use_vocab=False, pad_token=-1, batch_first=True)\n",
        "    fields[\"cond_col\"] = torchtext.data.Field(use_vocab=False,  pad_token=-1, include_lengths=True, batch_first=True)\n",
        "    fields[\"cond_op\"] = torchtext.data.Field(use_vocab=False,  pad_token=-1, batch_first=True)\n",
        "    fields[\"indices\"] = torchtext.data.Field(use_vocab=False, sequential=False, batch_first=True)\n",
        "    \n",
        "    field_list = [(k, fields[k]) for k in fields.keys()]\n",
        "    \n",
        "    return field_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6B1Z8VmueSqe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#READCODE!\n",
        "def construct_examples(data, fields):\n",
        "    '''\n",
        "    convert each data point into a example object in torchtext\n",
        "    '''\n",
        "    keys = [k[0] for k in fields if k[0] != 'indices']\n",
        "    data_cond = [dict((k, d[k]) for k in keys) for d in data]\n",
        "    examples = []\n",
        "    for i, ex in enumerate(data_cond):\n",
        "        # yield torchtext.data.Example.fromlist([ex[k] for k in keys] + [i], fields)\n",
        "        examples.append(torchtext.data.Example.fromlist([ex[k] for k in keys] + [i], fields))\n",
        "\n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QGDcmALq6kSy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3. Vocab\n",
        "\n",
        "Build and merge vocab for question and column name (`question_tok` and `col_seq` Fields) tokens:"
      ]
    },
    {
      "metadata": {
        "id": "TtWJcha4eSwn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def filter_counter(freqs, min_freq):\n",
        "    cnt = Counter()\n",
        "    for k, v in freqs.items():\n",
        "        if (min_freq is None) or (v >= min_freq):\n",
        "            cnt[k] = v\n",
        "    return cnt\n",
        "  \n",
        "def merge_vocabs(vocabs, min_freq=0, vocab_size=None):\n",
        "    \"\"\"\n",
        "    Merge individual vocabularies (assumed to be generated from disjoint\n",
        "    documents) into a larger vocabulary.\n",
        "\n",
        "    Args:\n",
        "        vocabs: `torchtext.vocab.Vocab` vocabularies to be merged\n",
        "        vocab_size: `int` the final vocabulary size. `None` for no limit.\n",
        "    Return:\n",
        "        `torchtext.vocab.Vocab`\n",
        "    \"\"\"\n",
        "    merged = Counter()\n",
        "    for vocab in vocabs:\n",
        "        merged += filter_counter(vocab.freqs, min_freq)\n",
        "    return torchtext.vocab.Vocab(merged,max_size=vocab_size, min_freq=min_freq)\n",
        "\n",
        "#READCODE!\n",
        "def build_vocab(train, dev, max_size, min_freq):\n",
        "    '''\n",
        "    build torchtext vocab for question_tok and col_seq fields, \n",
        "    and merge vocabs of the two\n",
        "    '''\n",
        "    fields = train.fields\n",
        "\n",
        "    merge_list = []\n",
        "    merge_name_list = (\"question_tok\", \"col_seq\")\n",
        "    for split in (dev, train,):\n",
        "        for merge_name_it in merge_name_list:\n",
        "            fields[merge_name_it].build_vocab(\n",
        "                split, max_size=max_size, min_freq=min_freq)\n",
        "            merge_list.append(fields[merge_name_it].vocab)\n",
        "\n",
        "    # need to know all the words to filter the pretrained word embeddings\n",
        "    merged_vocab = merge_vocabs(merge_list, vocab_size=max_size)\n",
        "    for merge_name_it in merge_name_list:\n",
        "        fields[merge_name_it].vocab = merged_vocab\n",
        "        \n",
        "    return merged_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Klq35uqJ6plV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 4. Load the data and create data iterators\n",
        "\n",
        "Torchtext then passes the Dataset to an Iterator. Iterators handle numericalizing, batching, packaging, and moving the data to the GPU. "
      ]
    },
    {
      "metadata": {
        "id": "nYN1Jjk0M4OB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#READCODE!\n",
        "def load_dataset(dataset_dir):\n",
        "    '''\n",
        "    load and process data into torchtext dataset, \n",
        "    build vocab and iterators\n",
        "    '''\n",
        "    with open(TABLE_PATH) as inf:\n",
        "        print(\"loading data from %s\"%TABLE_PATH)\n",
        "        table_data= json.load(inf)\n",
        "    \n",
        "    print(\"processing train and dev data...\")\n",
        "    train_data = process(TRAIN_PATH, table_data)\n",
        "    valid_data = process(DEV_PATH, table_data)\n",
        "    \n",
        "    print(\"building fields...\")\n",
        "    fields = get_fields()\n",
        "    \n",
        "    print(\"constructing examples...\")\n",
        "    train_examples = construct_examples(train_data, fields)\n",
        "    valid_examples = construct_examples(valid_data, fields)\n",
        "    \n",
        "    print(\"creating torchtext dataset...\")\n",
        "    valid = torchtext.data.Dataset(valid_examples, fields)\n",
        "    train = torchtext.data.Dataset(train_examples, fields)\n",
        "    \n",
        "    print(\"building vocab...\")\n",
        "    vocab = build_vocab(train, valid, max_size=50000, min_freq=1)\n",
        "    \n",
        "    print(\"creating iterators for train and dev...\")\n",
        "    train_iter, valid_iter = torchtext.data.Iterator.splits(\n",
        "        (train, valid), sort_key=lambda x: len(x.question_tok),\n",
        "        batch_sizes=(64, 64), device=device)\n",
        "    \n",
        "    print(\"done!\")\n",
        "      \n",
        "    return train_iter, valid_iter, vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b3sx7d2RM4Qj",
        "colab_type": "code",
        "outputId": "3ea6984c-9105-4e35-ec21-283d9bb81af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "train_iter, valid_iter, vocab = load_dataset(\"data/\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data from /content/gdrive/My Drive/nlp_hw4/data/tables.json\n",
            "processing train and dev data...\n",
            "building fields...\n",
            "constructing examples...\n",
            "creating torchtext dataset...\n",
            "building vocab...\n",
            "creating iterators for train and dev...\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YcnxDX5P6y5o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 5. Reconstruct inputs for the model\n",
        "\n",
        "Convert standardized Torchtext batched inputs to the inputs for SQLNet.\n",
        "\n",
        "**TODO**: reconstruct target label inputs for the condition module in SQLNet. You need to understand what the Torchtext standardized inputs look like."
      ]
    },
    {
      "metadata": {
        "id": "iK3o87_6Jpv-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#READCODE!\n",
        "#TODO!\n",
        "def reconstruct_input(cond_op, cond_col, col_concat, col_len):\n",
        "    '''\n",
        "    function to reconstruct column and target label input for SQLNet with torchtext\n",
        "    please check epoch_train function below to see where it gets called\n",
        "    \n",
        "    ---Parameters---\n",
        "    \n",
        "    cond_op (tensor): shape: (batch size, max where condition number in currect batch)\n",
        "           if no operation or condition, -1 padded, otherwise indices of\n",
        "           ('not', 'between', '=', '>', '<', '>=', '<=', '!=', 'in', 'like', 'is', 'exists')\n",
        "    cond_col (tensor): shape: (batch size, max where condition number in currect batch)\n",
        "           if no condition column, -1 padded, otherwise indices of columns\n",
        "    '''\n",
        "    #reconstruct column inputs for SQLNet\n",
        "    col_concat_list = col_concat.data.tolist()\n",
        "    col_len_list = col_len.data.tolist()\n",
        "    \n",
        "    col_name_len = [l for ls in col_len_list for l in ls if l != -1]\n",
        "    col_num = [len([l for l in ls if l != -1]) for ls in col_len_list]\n",
        "    col_B = len(col_name_len)\n",
        "    max_len = max(col_name_len)\n",
        "    col_emb_inds = np.zeros((col_B, max_len), dtype=np.int64)\n",
        "    cols_all = []\n",
        "    for col_concat_l, col_len_l in zip(col_concat_list, col_len_list):\n",
        "        start_ind = 0\n",
        "        end_ind = 0\n",
        "        for ll in col_len_l:\n",
        "            if ll == -1:\n",
        "                break\n",
        "            end_ind += ll\n",
        "            cols_all.append(col_concat_l[start_ind:end_ind])\n",
        "            start_ind += ll\n",
        "    \n",
        "    for i in range(col_B):\n",
        "        col_emb_inds[i,:len(cols_all[i])] = cols_all[i]\n",
        "    #vocab indices of words in all column names\n",
        "    col_ind_inp = torch.from_numpy(col_emb_inds).to(device)\n",
        "    \n",
        "    #TODO:IMPLEMENT YOUR CODE BELOW\n",
        "    #reconstruct target label inputs for SQLNet \n",
        "    #here for simplicity, we only do it for cond\n",
        "    #except cond, others are fake numbers: \n",
        "    #one example in ans_seq: [0, 1, cond_num, cond_col, cond_op, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
        "    ans_seq = []\n",
        "    cond_op_list = cond_op.data.tolist()\n",
        "    cond_col_list = cond_col.data.tolist()\n",
        "    for cond_op_l, cond_col_l in zip(cond_op_list, cond_col_list):\n",
        "      cond_op_cur = []\n",
        "      cond_col_cur = []\n",
        "      for l1 in cond_op_l:\n",
        "        if l1 != -1:\n",
        "          cond_op_cur.append(l1)\n",
        "      for l2 in cond_col_l:\n",
        "        if l2 != -1:\n",
        "          cond_col_cur.append(l2)\n",
        "      ans_seq.append([0, 1, len(cond_col_cur), cond_col_cur, cond_op_cur, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
        "      \n",
        "    return col_ind_inp, col_name_len, col_num, ans_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-xyMy8hnBV3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## you can uncomment this to see what a iterator looks like\n",
        "# for i, batch in enumerate(valid_iter):\n",
        "#     q, q_len = batch.question_tok\n",
        "#     col_concat, col_concat_len = batch.col_seq\n",
        "#     col_len = batch.col_len\n",
        "#     cond_op = batch.cond_op\n",
        "#     cond_col, cond_num = batch.cond_col\n",
        "    \n",
        "#     x_len = q_len.data.tolist()\n",
        "#     col_ind_inp, col_name_len, col_num, ans_seq= reconstruct_input(cond_op, cond_col, col_concat, col_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xC-WB4FW68-P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 6. Model training and testing\n",
        "\n",
        "Most of code is similar to the code in the last section but with modifications to accommodate the Torchtext data processing pipeline. "
      ]
    },
    {
      "metadata": {
        "id": "7ZSbMkXTpqH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SCAN\n",
        "class SQLNet(nn.Module):\n",
        "    '''\n",
        "    modified SQLNet with only cond module for demo\n",
        "    '''\n",
        "    def __init__(self, vocab, N_word, N_h=120, N_depth=2):\n",
        "        super(SQLNet, self).__init__()\n",
        "        self.N_h = N_h\n",
        "        self.N_depth = N_depth\n",
        "\n",
        "        self.max_col_num = 45\n",
        "        self.max_tok_num = 200\n",
        "        self.SQL_TOK = ['<UNK>', '<END>', 'WHERE', 'AND',\n",
        "                'EQL', 'GT', 'LT', '<BEG>']\n",
        "        self.COND_OPS = ['EQL', 'GT', 'LT']\n",
        "        \n",
        "        #MODIFIED: self embedding layer -> nn embedding layer\n",
        "#         vocab.load_vectors(\"glove.6B.50d\")\n",
        "        self.embed_layer = nn.Embedding(len(vocab), N_word)\n",
        "#         self.embed_layer.weight.data.copy_(vocab.vectors)\n",
        "\n",
        "        #Predict where condition\n",
        "        self.cond_pred = CondPredictor(N_word, N_h, N_depth)\n",
        "\n",
        "        self.CE = nn.CrossEntropyLoss()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, q_inds, q_len, col_inds, col_name_len, col_num,\n",
        "            gt_where=None, gt_cond=None, gt_sel=None):\n",
        "        \n",
        "        B = len(q_len)\n",
        "\n",
        "        sel_score = None\n",
        "        cond_score = None\n",
        "        group_score = None\n",
        "        order_score = None\n",
        "\n",
        "        q_emb_var = self.embed_layer(q_inds)\n",
        "        col_inp_var = self.embed_layer(col_inds)\n",
        "\n",
        "        cond_score = self.cond_pred(q_emb_var, q_len, col_inp_var, col_num, col_name_len, gt_cond=gt_cond)\n",
        "\n",
        "        return cond_score\n",
        "\n",
        "\n",
        "    def loss(self, score, truth_num):\n",
        "\n",
        "        cond_score = score\n",
        "\n",
        "        cond_num_score, cond_col_score, cond_op_score = cond_score\n",
        "\n",
        "        B = len(truth_num)\n",
        "        loss = 0\n",
        "        #----------------loss for cond_pred--------------------#\n",
        "        #cond_num_score, cond_col_score, cond_op_score = cond_score\n",
        "\n",
        "        #Evaluate the number of conditions\n",
        "        cond_num_truth = list(map(lambda x:x[2], truth_num))\n",
        "        cond_num_truth_var = torch.from_numpy(np.array(cond_num_truth)).to(device)\n",
        "        loss += self.CE(cond_num_score, cond_num_truth_var)\n",
        "        #Evaluate the columns of conditions\n",
        "        T = len(cond_col_score[0])\n",
        "        truth_prob = np.zeros((B, T), dtype=np.float32)\n",
        "        for b in range(B):\n",
        "            if len(truth_num[b][3]) > 0:\n",
        "                truth_prob[b][list(truth_num[b][3])] = 1\n",
        "\n",
        "        cond_col_truth_var = torch.from_numpy(truth_prob).to(device)\n",
        "        cond_col_prob = self.sigm(cond_col_score)\n",
        "        bce_loss = -torch.mean( 3*(cond_col_truth_var * \\\n",
        "                torch.log(cond_col_prob+1e-10)) + \\\n",
        "                (1-cond_col_truth_var) * torch.log(1-cond_col_prob+1e-10) )\n",
        "        loss += bce_loss\n",
        "        #Evaluate the operator of conditions\n",
        "        for b in range(len(truth_num)):\n",
        "            if len(truth_num[b][4]) == 0:\n",
        "                continue\n",
        "            cond_op_truth_var = torch.from_numpy(np.array(truth_num[b][4])).to(device)\n",
        "            cond_op_pred = cond_op_score[b, :len(truth_num[b][4])]\n",
        "            loss += (self.CE(cond_op_pred, cond_op_truth_var) \\\n",
        "                    / len(truth_num))\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def check_acc(self, pred_queries, gt_queries):\n",
        "        B = len(gt_queries)\n",
        "\n",
        "        tot_err = 0.0\n",
        "        cond_err = cond_num_err = cond_col_err = cond_op_err = 0.0\n",
        "        for b, pred_qry in enumerate(pred_queries):\n",
        "\n",
        "            good = True\n",
        "            tot_flag = True\n",
        "            cond_flag = True\n",
        "\n",
        "            # conds\n",
        "            cond_pred = pred_qry['conds']\n",
        "            flag = True\n",
        "            if len(cond_pred) != gt_queries[b][2]:\n",
        "                flag = False\n",
        "                cond_num_err += 1\n",
        "                cond_flag = False\n",
        "            if flag and set(x[0] for x in cond_pred) != set(gt_queries[b][3]):\n",
        "                flag = False\n",
        "                cond_col_err += 1\n",
        "                cond_flag = False\n",
        "            for idx in range(len(cond_pred)):\n",
        "                if not flag:\n",
        "                    break\n",
        "                gt_idx = gt_queries[b][3].index(cond_pred[idx][0])\n",
        "                if flag and gt_queries[b][4][gt_idx] != cond_pred[idx][1]:\n",
        "                    flag = False\n",
        "                    cond_op_err += 1\n",
        "                    cond_flag = False\n",
        "\n",
        "            if not cond_flag:\n",
        "                cond_err += 1\n",
        "                good = False\n",
        "\n",
        "            if not good:\n",
        "                tot_err += 1\n",
        "\n",
        "        return np.array((0, cond_err, 0, 0)), tot_err\n",
        "\n",
        "\n",
        "    def gen_query(self, score):\n",
        "\n",
        "        cond_score = score\n",
        "\n",
        "        cond_num_score, cond_col_score, cond_op_score = [x.data.cpu().numpy() if x is not None else None for x in cond_score]\n",
        "        ret_queries = []\n",
        "        B = len(cond_num_score)\n",
        "        for b in range(B):\n",
        "            cur_query = {}\n",
        "            #---------get cond predict\n",
        "            cur_query['conds'] = []\n",
        "            cond_num = np.argmax(cond_num_score[b])\n",
        "            max_idxes = np.argsort(-cond_col_score[b])[:cond_num]\n",
        "            for idx in range(cond_num):\n",
        "                cur_cond = []\n",
        "                cur_cond.append(max_idxes[idx])\n",
        "                cur_cond.append(np.argmax(cond_op_score[b][idx]))\n",
        "                cur_query['conds'].append(cur_cond)\n",
        "            ret_queries.append(cur_query)\n",
        "\n",
        "        return ret_queries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "reZ3FWQGR4gS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Torchtext handles data loading, processing, padding, and batching for you. \n",
        "\n",
        "**TODO**: Let's modify `epoch_train`  to use the Torchtext iterator object to batch the data and then train the model:"
      ]
    },
    {
      "metadata": {
        "id": "ODCws_s_jNGl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N_word = 50\n",
        "model = SQLNet(vocab, N_word=N_word).to(device)\n",
        "\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
        "\n",
        "#READCODE!\n",
        "#TODO!\n",
        "def epoch_train(model, optimizer, data_iter):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    #TODO:IMPLEMENT YOUR CODE BELOW\n",
        "    #iterate over data\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        #1. reconstruct inputs for SQLNet\n",
        "        q_inds, q_len = batch.question_tok\n",
        "        col_concat, col_concat_len = batch.col_seq\n",
        "        col_len = batch.col_len\n",
        "        cond_op = batch.cond_op\n",
        "        cond_col, cond_num = batch.cond_col\n",
        "        \n",
        "        x_len = np.array(q_len.data.tolist())\n",
        "        col_inds, col_name_len, col_num, ans_seq = reconstruct_input(cond_op, cond_col, col_concat, col_len)\n",
        "        col_name_len = np.array(col_name_len)\n",
        "        #2. run SQLNet forward pass (ingore gt_cond input for model.forward)\n",
        "        score = model.forward(q_inds, x_len, col_inds, col_name_len, col_num)\n",
        "        #3. compute loss, add it to total loss\n",
        "        loss = model.loss(score, ans_seq)\n",
        "        total_loss += loss.item()\n",
        "        #4. update weights in SQLNet\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        count += 1\n",
        "      \n",
        "    return total_loss / count\n",
        "\n",
        "\n",
        "def epoch_acc(model, data_iter):\n",
        "    model.eval()\n",
        "    one_acc_num = 0.0\n",
        "    tot_acc_num = 0.0\n",
        "    count = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        q_inds, q_len = batch.question_tok\n",
        "        col_concat, col_concat_len = batch.col_seq\n",
        "        col_len = batch.col_len\n",
        "        cond_op = batch.cond_op\n",
        "        cond_col, cond_num = batch.cond_col\n",
        "\n",
        "        x_len = np.array(q_len.data.tolist())\n",
        "        col_inds, col_name_len, col_num, ans_seq = reconstruct_input(cond_op, cond_col, col_concat, col_len)\n",
        "        col_name_len = np.array(col_name_len)\n",
        "        score = model.forward(q_inds, x_len, col_inds, col_name_len, col_num)\n",
        "        pred_queries = model.gen_query(score)\n",
        "        one_err, tot_err = model.check_acc(pred_queries, ans_seq)\n",
        "        one_acc_num += (len(ans_seq)-one_err)\n",
        "        tot_acc_num += (len(ans_seq)-tot_err)\n",
        "        \n",
        "        count += len(ans_seq)\n",
        "        \n",
        "    return tot_acc_num / count, one_acc_num / count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yKFgLqJOSzAL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's run train and evaluate model:"
      ]
    },
    {
      "metadata": {
        "id": "ULZKmNtDM4WD",
        "colab_type": "code",
        "outputId": "adb3bb7e-cbbf-4217-890c-3871cc38f248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3689
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=64\n",
        "#initial accuracy\n",
        "init_acc = epoch_acc(model, valid_iter)\n",
        "best_sel_acc = init_acc[1][0]\n",
        "best_cond_acc = init_acc[1][1]\n",
        "best_group_acc = init_acc[1][2]\n",
        "best_order_acc = init_acc[1][3]\n",
        "best_tot_acc = 0.0\n",
        "\n",
        "for i in range(30):\n",
        "    print('Epoch %d @ %s'%(i+1, datetime.datetime.now()))\n",
        "    print(' Loss = %s'%epoch_train(model, optimizer, train_iter))\n",
        "    train_tot_acc, train_bkd_acc = epoch_acc(model, train_iter)\n",
        "    print(' Train acc_qm: %s' % train_tot_acc)\n",
        "    print(' Breakdown results: sel: %s, cond: %s, group: %s, order: %s'\\\n",
        "        % (train_bkd_acc[0], train_bkd_acc[1], train_bkd_acc[2], train_bkd_acc[3]))\n",
        "\n",
        "    val_tot_acc, val_bkd_acc = epoch_acc(model, valid_iter)\n",
        "    print(' Dev acc_qm: %s' % val_tot_acc)\n",
        "    print(' Breakdown results: sel: %s, cond: %s, group: %s, order: %s'\\\n",
        "        % (val_bkd_acc[0], val_bkd_acc[1], val_bkd_acc[2], val_bkd_acc[3]))\n",
        "\n",
        "    #save models\n",
        "    if val_bkd_acc[0] > best_sel_acc:\n",
        "        best_sel_acc = val_bkd_acc[0]\n",
        "        print(\"Saving sel model...\")\n",
        "        torch.save(model.sel_pred.state_dict(), os.path.join(SAVED_MODEL_DIR, \"sel_models.dump\"))\n",
        "    if val_bkd_acc[1] > best_cond_acc:\n",
        "        best_cond_acc = val_bkd_acc[1]\n",
        "        print(\"Saving cond model...\")\n",
        "        torch.save(model.cond_pred.state_dict(), os.path.join(SAVED_MODEL_DIR, \"cond_models.dump\"))\n",
        "    if val_bkd_acc[2] > best_group_acc:\n",
        "        best_group_acc = val_bkd_acc[2]\n",
        "        print(\"Saving group model...\") \n",
        "        torch.save(model.group_pred.state_dict(), os.path.join(SAVED_MODEL_DIR, \"group_models.dump\"))\n",
        "    if val_bkd_acc[3] > best_order_acc:\n",
        "        best_order_acc = val_bkd_acc[3]\n",
        "        print(\"Saving order model...\")\n",
        "        torch.save(model.order_pred.state_dict(), os.path.join(SAVED_MODEL_DIR, \"order_models.dump\"))\n",
        "    if val_tot_acc > best_tot_acc:\n",
        "        best_tot_acc = val_tot_acc\n",
        "\n",
        "    print(' Best val sel = %s, cond = %s, group = %s, order = %s, tot = %s'%(best_sel_acc, best_cond_acc, best_group_acc, best_order_acc, best_tot_acc))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 @ 2019-04-09 23:06:53.816101\n",
            " Loss = 1.6299720948392695\n",
            " Train acc_qm: 0.451\n",
            " Breakdown results: sel: 1.0, cond: 0.451, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.4284332688588008\n",
            " Breakdown results: sel: 1.0, cond: 0.4284332688588008, group: 1.0, order: 1.0\n",
            "Saving cond model...\n",
            " Best val sel = 1.0, cond = 0.4284332688588008, group = 1.0, order = 1.0, tot = 0.4284332688588008\n",
            "Epoch 2 @ 2019-04-09 23:07:29.315893\n",
            " Loss = 1.2228886967355554\n",
            " Train acc_qm: 0.4677142857142857\n",
            " Breakdown results: sel: 1.0, cond: 0.4677142857142857, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.45551257253384914\n",
            " Breakdown results: sel: 1.0, cond: 0.45551257253384914, group: 1.0, order: 1.0\n",
            "Saving cond model...\n",
            " Best val sel = 1.0, cond = 0.45551257253384914, group = 1.0, order = 1.0, tot = 0.45551257253384914\n",
            "Epoch 3 @ 2019-04-09 23:08:05.989370\n",
            " Loss = 0.9800558380105279\n",
            " Train acc_qm: 0.5031428571428571\n",
            " Breakdown results: sel: 1.0, cond: 0.5031428571428571, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.488394584139265\n",
            " Breakdown results: sel: 1.0, cond: 0.488394584139265, group: 1.0, order: 1.0\n",
            "Saving cond model...\n",
            " Best val sel = 1.0, cond = 0.488394584139265, group = 1.0, order = 1.0, tot = 0.488394584139265\n",
            "Epoch 4 @ 2019-04-09 23:08:45.589590\n",
            " Loss = 0.8015877550298517\n",
            " Train acc_qm: 0.4908571428571429\n",
            " Breakdown results: sel: 1.0, cond: 0.4908571428571429, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.43617021276595747\n",
            " Breakdown results: sel: 1.0, cond: 0.43617021276595747, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.488394584139265, group = 1.0, order = 1.0, tot = 0.488394584139265\n",
            "Epoch 5 @ 2019-04-09 23:09:20.062205\n",
            " Loss = 0.6890134857459502\n",
            " Train acc_qm: 0.5252857142857142\n",
            " Breakdown results: sel: 1.0, cond: 0.5252857142857142, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.4941972920696325\n",
            " Breakdown results: sel: 1.0, cond: 0.4941972920696325, group: 1.0, order: 1.0\n",
            "Saving cond model...\n",
            " Best val sel = 1.0, cond = 0.4941972920696325, group = 1.0, order = 1.0, tot = 0.4941972920696325\n",
            "Epoch 6 @ 2019-04-09 23:09:53.472789\n",
            " Loss = 0.5944271301681345\n",
            " Train acc_qm: 0.5055714285714286\n",
            " Breakdown results: sel: 1.0, cond: 0.5055714285714286, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.4497098646034816\n",
            " Breakdown results: sel: 1.0, cond: 0.4497098646034816, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.4941972920696325, group = 1.0, order = 1.0, tot = 0.4941972920696325\n",
            "Epoch 7 @ 2019-04-09 23:10:30.747044\n",
            " Loss = 0.5115291935476389\n",
            " Train acc_qm: 0.539\n",
            " Breakdown results: sel: 1.0, cond: 0.539, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.4632495164410058\n",
            " Breakdown results: sel: 1.0, cond: 0.4632495164410058, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.4941972920696325, group = 1.0, order = 1.0, tot = 0.4941972920696325\n",
            "Epoch 8 @ 2019-04-09 23:11:05.984962\n",
            " Loss = 0.4660454653880813\n",
            " Train acc_qm: 0.5508571428571428\n",
            " Breakdown results: sel: 1.0, cond: 0.5508571428571428, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5164410058027079\n",
            " Breakdown results: sel: 1.0, cond: 0.5164410058027079, group: 1.0, order: 1.0\n",
            "Saving cond model...\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 9 @ 2019-04-09 23:11:39.504274\n",
            " Loss = 0.41105934273112904\n",
            " Train acc_qm: 0.553\n",
            " Breakdown results: sel: 1.0, cond: 0.553, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.48549323017408125\n",
            " Breakdown results: sel: 1.0, cond: 0.48549323017408125, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 10 @ 2019-04-09 23:12:13.468657\n",
            " Loss = 0.35107766457579354\n",
            " Train acc_qm: 0.5508571428571428\n",
            " Breakdown results: sel: 1.0, cond: 0.5508571428571428, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.45357833655706\n",
            " Breakdown results: sel: 1.0, cond: 0.45357833655706, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 11 @ 2019-04-09 23:12:49.390507\n",
            " Loss = 0.3157094577496702\n",
            " Train acc_qm: 0.5714285714285714\n",
            " Breakdown results: sel: 1.0, cond: 0.5714285714285714, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.4961315280464217\n",
            " Breakdown results: sel: 1.0, cond: 0.4961315280464217, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 12 @ 2019-04-09 23:13:25.891132\n",
            " Loss = 0.28595962910489603\n",
            " Train acc_qm: 0.5717142857142857\n",
            " Breakdown results: sel: 1.0, cond: 0.5717142857142857, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.4796905222437137\n",
            " Breakdown results: sel: 1.0, cond: 0.4796905222437137, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 13 @ 2019-04-09 23:13:58.792767\n",
            " Loss = 0.2633158468387344\n",
            " Train acc_qm: 0.5651428571428572\n",
            " Breakdown results: sel: 1.0, cond: 0.5651428571428572, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.46421663442940037\n",
            " Breakdown results: sel: 1.0, cond: 0.46421663442940037, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 14 @ 2019-04-09 23:14:36.115118\n",
            " Loss = 0.24034510986371474\n",
            " Train acc_qm: 0.5844285714285714\n",
            " Breakdown results: sel: 1.0, cond: 0.5844285714285714, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.48549323017408125\n",
            " Breakdown results: sel: 1.0, cond: 0.48549323017408125, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 15 @ 2019-04-09 23:15:13.736606\n",
            " Loss = 0.2076548949561336\n",
            " Train acc_qm: 0.5844285714285714\n",
            " Breakdown results: sel: 1.0, cond: 0.5844285714285714, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.4825918762088975\n",
            " Breakdown results: sel: 1.0, cond: 0.4825918762088975, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 16 @ 2019-04-09 23:15:48.001821\n",
            " Loss = 0.20878803431987764\n",
            " Train acc_qm: 0.5901428571428572\n",
            " Breakdown results: sel: 1.0, cond: 0.5901428571428572, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5096711798839458\n",
            " Breakdown results: sel: 1.0, cond: 0.5096711798839458, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 17 @ 2019-04-09 23:16:22.056218\n",
            " Loss = 0.18364378068257461\n",
            " Train acc_qm: 0.5985714285714285\n",
            " Breakdown results: sel: 1.0, cond: 0.5985714285714285, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5106382978723404\n",
            " Breakdown results: sel: 1.0, cond: 0.5106382978723404, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 18 @ 2019-04-09 23:16:58.934904\n",
            " Loss = 0.15556318566880442\n",
            " Train acc_qm: 0.5985714285714285\n",
            " Breakdown results: sel: 1.0, cond: 0.5985714285714285, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5009671179883946\n",
            " Breakdown results: sel: 1.0, cond: 0.5009671179883946, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 19 @ 2019-04-09 23:17:33.604766\n",
            " Loss = 0.14956041401760145\n",
            " Train acc_qm: 0.594\n",
            " Breakdown results: sel: 1.0, cond: 0.594, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.4796905222437137\n",
            " Breakdown results: sel: 1.0, cond: 0.4796905222437137, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5164410058027079, group = 1.0, order = 1.0, tot = 0.5164410058027079\n",
            "Epoch 20 @ 2019-04-09 23:18:07.243060\n",
            " Loss = 0.14819020351225679\n",
            " Train acc_qm: 0.6044285714285714\n",
            " Breakdown results: sel: 1.0, cond: 0.6044285714285714, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5319148936170213\n",
            " Breakdown results: sel: 1.0, cond: 0.5319148936170213, group: 1.0, order: 1.0\n",
            "Saving cond model...\n",
            " Best val sel = 1.0, cond = 0.5319148936170213, group = 1.0, order = 1.0, tot = 0.5319148936170213\n",
            "Epoch 21 @ 2019-04-09 23:18:44.226413\n",
            " Loss = 0.14263986992565067\n",
            " Train acc_qm: 0.6027142857142858\n",
            " Breakdown results: sel: 1.0, cond: 0.6027142857142858, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5019342359767892\n",
            " Breakdown results: sel: 1.0, cond: 0.5019342359767892, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5319148936170213, group = 1.0, order = 1.0, tot = 0.5319148936170213\n",
            "Epoch 22 @ 2019-04-09 23:19:19.441752\n",
            " Loss = 0.12060600700364872\n",
            " Train acc_qm: 0.6064285714285714\n",
            " Breakdown results: sel: 1.0, cond: 0.6064285714285714, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5077369439071566\n",
            " Breakdown results: sel: 1.0, cond: 0.5077369439071566, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5319148936170213, group = 1.0, order = 1.0, tot = 0.5319148936170213\n",
            "Epoch 23 @ 2019-04-09 23:19:52.706503\n",
            " Loss = 0.11311200384727933\n",
            " Train acc_qm: 0.6091428571428571\n",
            " Breakdown results: sel: 1.0, cond: 0.6091428571428571, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5222437137330754\n",
            " Breakdown results: sel: 1.0, cond: 0.5222437137330754, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5319148936170213, group = 1.0, order = 1.0, tot = 0.5319148936170213\n",
            "Epoch 24 @ 2019-04-09 23:20:25.984042\n",
            " Loss = 0.11894066696139899\n",
            " Train acc_qm: 0.608\n",
            " Breakdown results: sel: 1.0, cond: 0.608, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5019342359767892\n",
            " Breakdown results: sel: 1.0, cond: 0.5019342359767892, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5319148936170213, group = 1.0, order = 1.0, tot = 0.5319148936170213\n",
            "Epoch 25 @ 2019-04-09 23:21:01.366779\n",
            " Loss = 0.10459769310599024\n",
            " Train acc_qm: 0.6101428571428571\n",
            " Breakdown results: sel: 1.0, cond: 0.6101428571428571, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5164410058027079\n",
            " Breakdown results: sel: 1.0, cond: 0.5164410058027079, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5319148936170213, group = 1.0, order = 1.0, tot = 0.5319148936170213\n",
            "Epoch 26 @ 2019-04-09 23:21:37.655868\n",
            " Loss = 0.10622279268096793\n",
            " Train acc_qm: 0.6101428571428571\n",
            " Breakdown results: sel: 1.0, cond: 0.6101428571428571, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5077369439071566\n",
            " Breakdown results: sel: 1.0, cond: 0.5077369439071566, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5319148936170213, group = 1.0, order = 1.0, tot = 0.5319148936170213\n",
            "Epoch 27 @ 2019-04-09 23:22:14.770546\n",
            " Loss = 0.11214756264605305\n",
            " Train acc_qm: 0.6025714285714285\n",
            " Breakdown results: sel: 1.0, cond: 0.6025714285714285, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.4961315280464217\n",
            " Breakdown results: sel: 1.0, cond: 0.4961315280464217, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5319148936170213, group = 1.0, order = 1.0, tot = 0.5319148936170213\n",
            "Epoch 28 @ 2019-04-09 23:22:53.032182\n",
            " Loss = 0.1328181087293408\n",
            " Train acc_qm: 0.61\n",
            " Breakdown results: sel: 1.0, cond: 0.61, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5164410058027079\n",
            " Breakdown results: sel: 1.0, cond: 0.5164410058027079, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5319148936170213, group = 1.0, order = 1.0, tot = 0.5319148936170213\n",
            "Epoch 29 @ 2019-04-09 23:23:29.339405\n",
            " Loss = 0.10563933376900174\n",
            " Train acc_qm: 0.6174285714285714\n",
            " Breakdown results: sel: 1.0, cond: 0.6174285714285714, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5251450676982592\n",
            " Breakdown results: sel: 1.0, cond: 0.5251450676982592, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5319148936170213, group = 1.0, order = 1.0, tot = 0.5319148936170213\n",
            "Epoch 30 @ 2019-04-09 23:24:04.119532\n",
            " Loss = 0.10035321304405277\n",
            " Train acc_qm: 0.621\n",
            " Breakdown results: sel: 1.0, cond: 0.621, group: 1.0, order: 1.0\n",
            " Dev acc_qm: 0.5222437137330754\n",
            " Breakdown results: sel: 1.0, cond: 0.5222437137330754, group: 1.0, order: 1.0\n",
            " Best val sel = 1.0, cond = 0.5319148936170213, group = 1.0, order = 1.0, tot = 0.5319148936170213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MrBEH4vFM4Yv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VPoiEEfQ7bYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "\n",
        "Now that you have completed the assignment, follow the steps below to submit your aissgnment:\n",
        "1. Click __Runtime__  > __Run all__ to generate the output for all cells in the notebook.\n",
        "2. Save the notebook with the output from all the cells in the notebook by click __File__ > __Download .ipynb__.\n",
        "3. Copy model train and test prints, answers to all short questions, and the shareable line of this notebook to a `README.txt` file.\n",
        "4. Put the .ipynb file and `README.txt` under your hidden directory on the Zoo server `~/hidden/<YOUR_PIN>/Homework4/`.\n",
        "5. As a final step, run a script that will set up the permissions to your homework files, so we can access and run your code to grade it. Make sure the command be;pw runs without errors, and do not make any changes or run the code again. If you do run the code again or make any changes, you need to run the permissions script again. Submissions without the correct permissions may incur some grading penalty.\n",
        "`/home/classes/cs477/bash_files/hw4_set_permissions.sh <YOUR_PIN>`"
      ]
    },
    {
      "metadata": {
        "id": "P-IlJZyhABUa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p1Lo7DcfHGG-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}