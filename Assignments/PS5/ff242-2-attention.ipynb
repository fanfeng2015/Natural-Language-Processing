{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ff242-2-attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9gqdR86trj42",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Introduction\n",
        "\n",
        "A reminder of what our initial model looked like:\n",
        "<center><img src=\"https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/master/assets/seq2seq1.png\" alt=\"mlp\" align=\"middle\"></center>\n",
        "\n",
        "In this architecture, our context vector still needs to contain all of the information about the source sentence. The model implemented in this notebook avoids this compression by allowing the decoder to look at the entire source sentence (via its hidden states) at each decoding step! How does it do this? It uses *attention*. \n",
        "\n",
        "Attention works by first, calculating an attention vector, $a$, that is the length of the source sentence. The attention vector has the property that each element is between 0 and 1, and the entire vector sums to 1. We then calculate a weighted sum of our source sentence hidden states, $H$, to get a weighted source vector, $w$. \n",
        "\n",
        "$$w = \\sum_{i}a_ih_i$$\n",
        "\n",
        "We calculate a new weighted source vector every time-step when decoding, using it as input to our decoder RNN as well as the linear layer to make a prediction. We'll explain how to do all of this during the tutorial.\n",
        "\n",
        "## Preparing Data\n",
        "\n",
        "The preparation is the same as last time. **TODO** Please copy over your completed code in the areas marked TODO from the previous notebook."
      ]
    },
    {
      "metadata": {
        "id": "c7rQqWpcrffA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "SEED = 1\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.enabled = False \n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZB8Pz-iCrffC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! python -m spacy download en\n",
        "! python -m spacy download de\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0juzzofXrffE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9da3a66a-4d83-440c-b556-5a65bef373de"
      },
      "cell_type": "code",
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# TODO fill in from the previous notebook\n",
        "BOS_WORD = '<sos>'\n",
        "EOS_WORD = '<eos>'\n",
        "SRC = Field(tokenize=tokenize_de, init_token=BOS_WORD, eos_token=EOS_WORD, lower=True, include_lengths=True)\n",
        "TRG = Field(tokenize=tokenize_en, init_token=BOS_WORD, eos_token=EOS_WORD, lower=True)\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))\n",
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(train_data, min_freq=2)\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# TODO complete from the previou snotebook\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device, sort_key = lambda x : len(x.src), sort_within_batch=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:02<00:00, 584kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 165kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 160kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "K0o2V4zNlOIJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TODO** The encoder is the same as part 1. Please fill in the code below"
      ]
    },
    {
      "metadata": {
        "id": "_Zo_6c2VrffQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        # TODO uncomment and fill in \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        # TODO uncomment and fill in \n",
        "        self.fc = nn.Linear(2 * enc_hid_dim, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "        # src = [src sent len, batch size]\n",
        "        # src_len = [src sent len]\n",
        "  \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded = [src sent len, batch size, emb dim]\n",
        "      \n",
        "        # TODO uncomment and fill in with correct padded sequence function\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
        "        \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        # TODO uncomment and fill in with correct padded sequence function               \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "        # outputs = [sent len, batch size, dec_hid_dim * num directions]\n",
        "        # hidden = [n layers * num directions, batch size, dec_hid_dim]\n",
        "        \n",
        "        # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        # outputs are always from the last layer\n",
        "        \n",
        "        # hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        # hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        # initial decoder hidden is final hidden state of the forwards and backwards encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        \n",
        "        # outputs = [sent len, batch size, enc hid dim * 2]\n",
        "        # hidden = [batch size, dec_hid_dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p9OvRPyjs5nK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attention\n",
        "\n",
        "\n",
        "I would strongly recommend reading through [this tutorial](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) on attention (You can skip the parts on neural turing machines and self-attentive gans). We will be implementing the additive attention mentioned in that tutorial and which is described below. \n",
        "\n",
        "<br/>\n",
        "\n",
        "The goal of the attention layer is to help the model better decide which tokens from the input are most important at the current decoding timestep. The attention layer will take in the previous hidden state of the decoder, $s_{t-1}$, and all of the stacked forward and backward hidden states from the encoder, $H$. The layer will output an attention vector, $a_t$, that is the length of the source sentence, each element is between 0 and 1 and the entire vector sums to 1. Intuitively, this layer takes what we have decoded so far, $s_{t-1}$, and all of what we have encoded, $H$, to produce a vector, $a_t$, that represents which words in the source sentence we should pay the most attention to in order to correctly predict the next word to decode, $\\hat{y}_{t+1}$. \n",
        "\n",
        "<br/>\n",
        "\n",
        "First, we calculate the *energy* between the previous decoder hidden state and the encoder hidden states. As our encoder hidden states are a sequence of $T$ tensors, and our previous decoder hidden state is a single tensor, the first thing we do is `repeat` the previous decoder hidden state $T$ times. We then calculate the energy, $E_t$, between them by concatenating them together and passing them through a linear layer (`attn`) and a $\\tanh$ activation function. \n",
        "\n",
        "$$E_t = \\tanh(\\text{attn}(s_{t-1}, H))$$ \n",
        "\n",
        "This can be thought of as calculating how well each encoder hidden state \"matches\" the previous decoder hidden state.\n",
        "\n",
        "We currently have a **[dec hid dim, src sent len]** tensor for each example in the batch. We want this to be **[src sent len]** for each example in the batch, as the attention should be over the length of the source sentence. This is achieved by multiplying the `energy` by a **[1, dec hid dim]** tensor, $v$.\n",
        "\n",
        "$$\\hat{a}_t = v E_t$$\n",
        "\n",
        "We can think of this as calculating a weighted sum of the \"match\" over all `dec_hid_dem` elements for each encoder hidden state, where the weights are learned (as we learn the parameters of $v$).\n",
        "\n",
        "Finally, we ensure the attention vector fits the constraints of having all elements between 0 and 1 and the vector summing to 1 by passing it through a $\\text{softmax}$ layer.\n",
        "\n",
        "$$a_t = \\text{softmax}(\\hat{a_t})$$\n",
        "\n",
        "This gives us the attention over the source sentence!\n",
        "\n",
        "Graphically, this looks something like below. This is for calculating the very first attention vector, where $s_{t-1} = s_0 = z$. The green/yellow blocks represent the hidden states from both the forward and backward RNNs, and the attention computation is all done within the pink block.\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/master/assets/seq2seq9.png\" alt=\"mlp\" align=\"middle\"></center>\n",
        "\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "#### Implementation\n",
        "\n",
        "**TODO**: \n",
        "Here you will implement the attention mechanism. Notice how we will pass a mask to the attention layer. When we were writing the torchtext code, we decided to use \"include_lengths\" in the source field to keep track of the varied lengths of all the source examples and to pad them accordingly. Now we want to use that information to \"mask,\" or set to zero, those entries in the source tensor which correspond to padding and not actual input. We don't want to pay attention to padded zeros! \n",
        "\n",
        "In the implementation below, *hidden* is equal to $s_{t-1}$ refereneced above and *encoder_outputs* is equivalent to the H matrix. Sizes of these tensors are specified in the code. There is a **TODO** flag in the line right before every line which you have to modify in the code and are numbered accordingly: \n",
        "\n",
        "\n",
        "1.  For the *hidden* tensor, add a dimension between batch_size and dec_hid_dim and  repeat this dimension for src_len times. Look at the [unsqueeze](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unsqueeze) and [repeat](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat) functions in torch documentation. \n",
        "\n",
        "2. Currently the dimensions of encoder_outputs are (src sent len, batch size, enc hid dim x 2) but we ultimately want the dimensions to be (batch size, src sent len, enc hid dim x 2) for matrix multiplication so use the [permute function](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute) to switch these dimensions. \n",
        "\n",
        "\n",
        "3.  Energy is currently this size: (batch size, src sent len, dec hid dim) and we want it to be of size (batch size, dec hid dim, src sent len). Use the permute function to swap the dimensions. \n",
        "\n",
        "4. Matrix multiple v and energy and then squeeze the middle dimension. Look at [torch.mm](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.mm) and [torch.bmm](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.bmm) and determine which is appropriate to use for matrix multiplication here. "
      ]
    },
    {
      "metadata": {
        "id": "whVVU53krffT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        # hidden = [batch size, dec hid dim]\n",
        "        # encoder_outputs = [src sent len, batch size, enc hid dim * 2] \n",
        "        #   -- it contains the hidden states (bidirectional) over the entire src sentence\n",
        "        # mask = [batch size, src sent len]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        # repeat encoder hidden state src_len times\n",
        "        # TODO 1:\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        # TODO 2:\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        # hidden = [batch size, src sent len, dec hid dim]\n",
        "        # encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        # This implements the equation for energy in the notes above\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        # energy = [batch size, src sent len, dec hid dim]\n",
        "        \n",
        "        # TODO 3: \n",
        "        energy = energy.permute(0, 2, 1)\n",
        "        # energy = [batch size, dec hid dim, src sent len]\n",
        "        \n",
        "        # v = [dec hid dim]\n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)         # repeat v to allow for matrix multiplication\n",
        "        # v = [batch size, 1, dec hid dim]\n",
        "        \n",
        "        # TODO 4: \n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "        \n",
        "        # attention = [batch size, src sent len]\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)   # mask the padded input indices\n",
        "        \n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0KJdiLhtC3T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "\n",
        "Next up is the decoder. \n",
        "\n",
        "The decoder contains the attention layer, `attention`, which takes the previous hidden state, $s_{t-1}$, all of the encoder hidden states, $H$, and returns the attention vector, $a_t$.\n",
        "\n",
        "We then use this attention vector to create a weighted source vector, $w_t$, denoted by `weighted`, which is a weighted sum of the encoder hidden states, $H$, using $a_t$ as the weights.\n",
        "\n",
        "$$w_t = a_t H$$\n",
        "\n",
        "The input word (that has been embedded), $y_t$, the weighted source vector, $w_t$, and the previous decoder hidden state, $s_{t-1}$, are then all passed into the decoder RNN, with $y_t$ and $w_t$ being concatenated together.\n",
        "\n",
        "$$s_t = \\text{DecoderGRU}(y_t, w_t, s_{t-1})$$\n",
        "\n",
        "We then pass $y_t$, $w_t$ and $s_t$ through the linear layer, $f$, to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$. This is done by concatenating them all together.\n",
        "\n",
        "$$\\hat{y}_{t+1} = f(y_t, w_t, s_t)$$\n",
        "\n",
        "The image below shows decoding the first word in an example translation.\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/master/assets/seq2seq10.png\" alt=\"mlp\" align=\"middle\"></center>\n",
        "\n",
        "The green/yellow blocks show the forward/backward encoder RNNs which output $H$, the red block shows the context vector, $z = h_T = \\tanh(g(h^\\rightarrow_T,h^\\leftarrow_T)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$, the blue block shows the decoder RNN which outputs $s_t$, the purple block shows the linear layer, $f$, which outputs $\\hat{y}_{t+1}$ and the orange block shows the calculation of the weighted sum over $H$ by $a_t$ and outputs $w_t$. Not shown is the calculation of $a_t$."
      ]
    },
    {
      "metadata": {
        "id": "DUogrFnLf9Un",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TODO**\n",
        "\n",
        "As above,  the TODO in the decoder code are numbered: \n",
        "\n",
        "1. Using the same function for matrix multiplication as above, matrix multiple *a* and *encoder_outputs*. Also, in your README, reiterate how **a** from the equations above can be understood as encoding the importance of words in the source and how this matrix multiplication incorporates this into the modified hidden state (put this as Q3).\n",
        "\n",
        "2. Concatenate *embedded* and *weighted* along their last dimension. Look at the torch function [*cat*](https://pytorch.org/docs/stable/torch.html#torch.cat). \n"
      ]
    },
    {
      "metadata": {
        "id": "RhynBEmerffV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "        # input = [batch size]\n",
        "        # hidden = [batch size, dec hid dim]\n",
        "        # encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        # mask = [batch size, src sent len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        # input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "        # a = [batch size, src sent len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        # a = [batch size, 1, src sent len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        # TODO 1: \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        # weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        # weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        # TODO 2: \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        # rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        # output = [sent len, batch size, dec hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        # sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        # output = [1, batch size, dec hid dim]\n",
        "        # hidden = [1, batch size, dec hid dim]\n",
        "        # this also means that output == hidden\n",
        "        assert (output == hidden).all(), print(output.shape, hidden.shape, output[0,0,:25], hidden[0,0,:25])\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        output = self.out(torch.cat((output, weighted, embedded), dim=1))\n",
        "        \n",
        "        # output = [bsz, output dim]\n",
        "        \n",
        "        return output, hidden.squeeze(0), a.squeeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pMW2OqcTtKSk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq\n",
        "\n",
        "This seq2seq encapsulator is similar to the last notebook, except that we also return attention scores with the decoder. Also, we slightly refactored the code to allow an inference mode (when we want to predict in a real-world setting and return as soon as we predict the end-of-sentence token).\n",
        "\n",
        "Briefly going over all of the decoding steps:\n",
        "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive $z$ and $H$\n",
        "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n",
        "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n",
        "- we then decode within a loop:\n",
        "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and all encoder outputs, $H$, into the decoder\n",
        "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        "  - we then decide if we are going to teacher force or not, setting the next input as appropriate"
      ]
    },
    {
      "metadata": {
        "id": "yFNxav4zrffY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, pad_idx, sos_idx, eos_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "        self.sos_idx = sos_idx\n",
        "        self.eos_idx = eos_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
        "        # src = [src sent len, batch size]\n",
        "        # src_len = [batch size]\n",
        "        # trg = [trg sent len, batch size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        if trg is None:\n",
        "            inference = True\n",
        "            assert teacher_forcing_ratio == 0, \"Must be zero during inference\"\n",
        "            trg = torch.zeros((100, src.shape[1]), dtype=torch.long).fill_(self.sos_idx).to(src.device)\n",
        "        else:\n",
        "            inference = False\n",
        "            \n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # tensor to store attention\n",
        "        attentions = torch.zeros(max_len, batch_size, src.shape[0]).to(self.device)\n",
        "        \n",
        "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "                \n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        output = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)      \n",
        "        # mask = [batch size, src sent len]\n",
        "                \n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, attention = self.decoder(output, hidden, encoder_outputs, mask)\n",
        "            outputs[t] = output\n",
        "            attentions[t] = attention\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "            if inference and output.item() == self.eos_idx:\n",
        "                return outputs[:t], attentions[:t]\n",
        "            \n",
        "        return outputs, attentions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJu7WggGrffa",
        "colab_type": "code",
        "outputId": "7e81d098-d9f8-437f-a086-09034cc264a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "PAD_IDX = SRC.vocab.stoi['<pad>']\n",
        "SOS_IDX = TRG.vocab.stoi['<sos>']\n",
        "EOS_IDX = TRG.vocab.stoi['<eos>']\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, PAD_IDX, SOS_IDX, EOS_IDX, device).to(device)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "count_parameters(model)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20518405"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "yQ-CGTx1rffe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sizNa94Lrffg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pad_idx = TRG.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XBGEwgqdrffj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        src, src_len = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(src, src_len, trg)\n",
        "        loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1govJlfIrffm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, src_len = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, src_len, trg, 0) # turn off teacher forcing\n",
        "            loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T67gQMTqi8-P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TODO** Train the model for 5 epochs and then report your results in the README"
      ]
    },
    {
      "metadata": {
        "id": "rqIJS9kVrffp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "eb4217ea-5a9c-4ac6-b16e-2e14e9f0f9b7"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "  \n",
        "N_EPOCHS = 5\n",
        "CLIP = 10\n",
        "SAVE_DIR = 'models'\n",
        "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'tut4_model.pt')\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "if not os.path.isdir(f'{SAVE_DIR}'):\n",
        "    os.makedirs(f'{SAVE_DIR}')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 47s\n",
            "\tTrain Loss: 4.282 | Train PPL:  72.389\n",
            "\t Val. Loss: 3.728 |  Val. PPL:  41.590\n",
            "Epoch: 02 | Time: 0m 48s\n",
            "\tTrain Loss: 3.120 | Train PPL:  22.637\n",
            "\t Val. Loss: 3.422 |  Val. PPL:  30.629\n",
            "Epoch: 03 | Time: 0m 50s\n",
            "\tTrain Loss: 2.665 | Train PPL:  14.364\n",
            "\t Val. Loss: 3.299 |  Val. PPL:  27.093\n",
            "Epoch: 04 | Time: 0m 49s\n",
            "\tTrain Loss: 2.381 | Train PPL:  10.811\n",
            "\t Val. Loss: 3.173 |  Val. PPL:  23.885\n",
            "Epoch: 05 | Time: 0m 49s\n",
            "\tTrain Loss: 2.155 | Train PPL:   8.630\n",
            "\t Val. Loss: 3.246 |  Val. PPL:  25.691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mtaBgZLkrffs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7a1a119-8710-4a62-8a47-9963bca37ede"
      },
      "cell_type": "code",
      "source": [
        "SAVE_DIR = 'models'\n",
        "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'tut4_model.pt')))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.209 | Test PPL:  24.746 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nn5wXmTkisyb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference and Visualizing Attention"
      ]
    },
    {
      "metadata": {
        "id": "YKokgLERjep3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After you've trained your model, you want to get an idea of how well it's doing by seeing actual translations! Please take a look at the translate_sentence function, which takes a German sentence as input and returns the translation and attention scores. The attention scores correspond to the scores at each time step for every word in the input. This allows you to visualize what words the model was weighting the most during decoding and thus offers some notion of interpretability (you can see what is important for your model's predictions). This is often visualized as a heatmap (see example below).The code is commented to guide you. "
      ]
    },
    {
      "metadata": {
        "id": "9qRfVRUSrffu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence):\n",
        "    tokenized = tokenize_de(sentence) # tokenize sentence\n",
        "    tokenized = ['<sos>'] + [t.lower() for t in tokenized] + ['<eos>'] # add <sos> and <eos> tokens and lowercase\n",
        "    numericalized = [SRC.vocab.stoi[t] for t in tokenized] # convert tokens into indexes\n",
        "    sentence_length = torch.LongTensor([len(numericalized)]).to(device) # need sentence length for masking\n",
        "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device) # convert to tensor and add batch dimension\n",
        "    translation_tensor_probs, attention = model(tensor, sentence_length, None, 0) # pass through model to get translation probabilities\n",
        "    translation_tensor = torch.argmax(translation_tensor_probs.squeeze(1), 1) # get translation from highest probabilities\n",
        "    translation = [TRG.vocab.itos[t] for t in translation_tensor][1:] # ignore the first token, just like we do in the training loop\n",
        "    return translation, attention[1:] # ignore first attention array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYmhbAKWrffw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def display_attention(candidate, translation, attention):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    attention = attention[:len(translation)].squeeze(1).cpu().detach().numpy() # cut attention to same length as translation\n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in tokenize_de(candidate)] + ['<eos>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + translation)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nMzFhjs8kf3z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "1. Find a German sentence from Google Translate (or any other source) and try translating it to English with our system. Add the model's translation as well as Google's translation to your README. We are training for a very short time and on very little data, so we can't expect good results, but see if what parts our model is able to correctly translate. \n",
        "\n",
        "2. Use display_attention to visualize the attention heatmap for your example. "
      ]
    },
    {
      "metadata": {
        "id": "H5rEq5UKrff6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "705996a8-3eef-4683-b2b8-0e776cda50de"
      },
      "cell_type": "code",
      "source": [
        "# candidate = ' '.join(vars(valid_data.examples[0])['src'])\n",
        "# candidate_translation = ' '.join(vars(valid_data.examples[0])['trg'])\n",
        "candidate = \"Die Katze wurde vom Hund gejagt\"\n",
        "candidate_translation = \"the cat was chased by the dog\"\n",
        "\n",
        "print(candidate)\n",
        "print(candidate_translation)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Die Katze wurde vom Hund gejagt\n",
            "the cat was chased by the dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ggAzKhvjlHBi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The heamap below shows German along the x-axis and the model's English translation along the y axis. Each row shows you what words in the German sentence our system was giving the most attention weight to as a particular step in decoding. In the example below, you can see that our model correctly assigns weigth to the German word *gruppe* when decoding *a group*."
      ]
    },
    {
      "metadata": {
        "id": "zt8bYjfwrff8",
        "colab_type": "code",
        "outputId": "bacbaa38-91a8-487f-a42d-5710f16d95b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "cell_type": "code",
      "source": [
        "translation, attention = translate_sentence(candidate)\n",
        "print(translation)\n",
        "display_attention(candidate, translation, attention)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'cat', 'is', 'swimming', 'from', 'the', 'dog', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAEWCAYAAAA5Am/SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "hnfUNBtXjKhr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "\n",
        "Now that you have completed the assignment, follow the steps below to submit your aissgnment:\n",
        "1. Click __Runtime__  > __Run all__ to generate the output for all cells in the notebook.\n",
        "2. Save the notebook with the output from all the cells in the notebook by click __File__ > __Download .ipynb__.\n",
        "3. Copy model train and test prints, answers to all short questions, and the shareable link of this notebook to a `README.txt` file.\n",
        "4. Put the .ipynb file and `README.txt` under your hidden directory on the Zoo server `~/hidden/<YOUR_PIN>/Homework5/`.\n",
        "5. As a final step, run a script that will set up the permissions to your homework files, so we can access and run your code to grade it. Make sure the command runs without errors, and do not make any changes or run the code again. If you do run the code again or make any changes, you need to run the permissions script again. Submissions without the correct permissions may incur some grading penalty.\n",
        "`/home/classes/cs477/bash_files/hw5_set_permissions.sh <YOUR_PIN>`"
      ]
    },
    {
      "metadata": {
        "id": "P5Sc1SMNjK1p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}