{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ff242-3-transformer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "IcWQsDm3u31c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transformer Models\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "bRDq1l8K12NO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Transformer models took the NLP community by storm by achieving state-of-the-art results in machine translation in the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). A very good tutorial on this architecture can be found [here](http://jalammar.github.io/illustrated-transformer/). As you can tell from the name, this model is based on attention! It replaces standard recurrent neural networks used for translation with a self-attention-based network. \n",
        "\n",
        "The model is an encoder-decoder model, as shown below:\n",
        "\n",
        "\n",
        "<center><img src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png\" alt=\"mlp\" align=\"middle\"></center>\n",
        "\n",
        "The encoder and decoder each consists of modules which are repeated N times (N=6 in the original paper). Words are transformed into embeddings before being input into these modules. However, these embeddings are added with positional embeddings, which give the model a notion of relative input position (remember that there is no recurrence model which keeps track sequentially.). These positional embeddings consist of the sine and cosine functions of different frequencies: $$PE(pos, 2i) = sin(pos/10000^\\frac{2i}{d_{model}})$$   $$PE(pos, 2i+1) = cos(pos/10000^\\frac{2i}{d_{model}}).$$ Thus, each dimension of the positional encoding\n",
        "corresponds to a sinusoid.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Tbydiepg4qTx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The encoder consists of a self-attention layer (multi-head attention in the first figure) which is then followed by a feed-forward network. \n",
        "<center><img src=\"http://jalammar.github.io/images/t/encoder_with_tensors_2.png\" alt=\"mlp\" align=\"middle\"></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "The self-attention matrix calculation is shown below: \n",
        "\n",
        "<center><img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" alt=\"mlp\" align=\"middle\"></center>\n",
        "\n",
        "\n",
        "See the tutorial for a more in-depth explanation. On a higher level, the K matrix (K=Keys), refers to each word in the sentence. Then for the Q matrix (Q=Query) you want to use every word in the sentence as a query for that keyword and check its relevance for representing that key. This is divided by a constant and then softmax'ed before being multiplied by a matrix V (V=Value) which can be thought of as the *weighted* from the previous notebooks. The result of these calculations is a matrix called z: \n",
        "\n",
        "**TODO** In your readme file, please explain what the role of the denominator in the self-attention equation is (check the original paper). \n",
        "\n",
        "**Answer:** dk represents the dimensionality of the queries and keys. The dot-product attention is scaled by 1/sqrt(dk) because even though the dot-product attention is more efficient in time and space than the additive attention in practice (because of highly optimized matrix multiplication), it performs poorer than the additive attention if not scaled by larger values of dk. This is because for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. The role of the denominator is to counteract the above effect.\n",
        "\n",
        "The paper then introduces multiple z matrices (8 in the original paper). \n",
        "\n",
        "**TODO** In your readme file, please explain what the motivation is behind using multiple z matrices. \n",
        "\n",
        "**Answer:** This is referred to as multi-head attention. The motivation is that it allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
        "\n",
        "You may have noticed in the first figure above that there is an extra input arrow pointing to the *Add & Norm* module. This is called a residual connection. \n",
        "\n",
        "**TODO** In your readme file, explain what the benefits of using residual connections are here (and in neural networks in general). \n",
        "\n",
        "**Answer:** Residual connections allow gradients to flow through the network directly, without passing through non-linear activation functions. This prevents the gradients from exploding or vanishing."
      ]
    },
    {
      "metadata": {
        "id": "Wg_kKg-E7GJf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\n",
        "This is basically the same as before, with some slight modifications. "
      ]
    },
    {
      "metadata": {
        "id": "zSJWqK3Iu31e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_WTYF9LOu31h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SEED = 1\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.enabled = False \n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdCV26EBu31j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! python -m spacy download en\n",
        "! python -m spacy download de\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0HtUvcex0Jsz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6YlUMEC-0MM2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
        "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iQKIIwA_0OB0",
        "colab_type": "code",
        "outputId": "5bf5331f-81c2-40d7-90a6-40af4909006b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:02<00:00, 512kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 167kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 159kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "etujX3wd0QRQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(train_data, min_freq=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4TeV39w70R1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W1I44X0L0UCC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "     batch_size=BATCH_SIZE,\n",
        "     device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NWwPXHtM1KNq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TODO** \n",
        "\n",
        "1. Apply embeddings over the source, scale these embeddings, add positional embeddings and then at the end apply dropout to everything. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YZZp5Kpou31w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, encoder_layer, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pf_dim = pf_dim\n",
        "        self.encoder_layer = encoder_layer\n",
        "        self.self_attention = self_attention\n",
        "        self.positionwise_feedforward = positionwise_feedforward\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([encoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        # src = [batch size, src sent len]\n",
        "        # src_mask = [batch size, src sent len]\n",
        "        \n",
        "        pos = torch.arange(0, src.shape[1]).unsqueeze(0).repeat(src.shape[0], 1).to(self.device)\n",
        "        # TODO create position-aware embeddings\n",
        "        src = self.do((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        # src = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yx6-BA3vu311",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.ln = nn.LayerNorm(hid_dim)\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        # src = [batch size, src sent len, hid dim]\n",
        "        # src_mask = [batch size, src sent len]\n",
        "        src = self.ln(src + self.do(self.sa(src, src, src, src_mask)))\n",
        "        src = self.ln(src + self.do(self.pf(src)))\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ViyHOkd747Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TODO**\n",
        "\n",
        "1. Modify the size of the Q, K and V matrices to be of size (batch size, n heads, sent len, hid dim // n heads). You will find the view and permute functions from torch helpful. This line will be the same for each of the three matrices. \n",
        "\n",
        "2. Matrix multiple Q and K and scale the output following the equation above. \n",
        "\n",
        "3. Matrix multiple attention and V\n",
        "\n",
        "4. Change the shape of x to match the desired output shape. "
      ]
    },
    {
      "metadata": {
        "id": "hIo_9JySu312",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        bsz = query.shape[0]\n",
        "        \n",
        "        # query = key = value [batch size, sent len, hid dim]\n",
        "                \n",
        "        Q = self.w_q(query)\n",
        "        K = self.w_k(key)\n",
        "        V = self.w_v(value)\n",
        "        \n",
        "        # Q, K, V = [batch size, sent len, hid dim]\n",
        "        # TODO 1:\n",
        "        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        K = K.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        V = V.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        # Q, K, V = [batch size, n heads, sent len, hid dim // n heads]\n",
        "        \n",
        "        # TODO 2: \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        # energy = [batch size, n heads, sent len, sent len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = self.do(F.softmax(energy, dim=-1))\n",
        "        # attention = [batch size, n heads, sent len, sent len]\n",
        "        \n",
        "        # TODO 3: \n",
        "        x = torch.matmul(attention, V)\n",
        "        # x = [batch size, n heads, sent len, hid dim // n heads]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        # x = [batch size, sent len, n heads, hid dim // n heads]\n",
        "        \n",
        "        # TODO 4\n",
        "        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n",
        "        # x = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        # x = [batch size, sent len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pFMYjYrdu314",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedforward(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.pf_dim = pf_dim\n",
        "        \n",
        "        self.fc_1 = nn.Conv1d(hid_dim, pf_dim, 1)\n",
        "        self.fc_2 = nn.Conv1d(pf_dim, hid_dim, 1)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x = [batch size, sent len, hid dim]\n",
        "        x = x.permute(0, 2, 1)\n",
        "        # x = [batch size, hid dim, sent len]\n",
        "        \n",
        "        x = self.do(F.relu(self.fc_1(x)))\n",
        "        # x = [batch size, ff dim, sent len]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        # x = [batch size, hid dim, sent len]\n",
        "        \n",
        "        x = x.permute(0, 2, 1)\n",
        "        # x = [batch size, sent len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VpJ-zqyq7-we",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TODO** \n",
        "\n",
        "1. (same as above) -- Apply embeddings over the source, scale these embeddings, add positional embeddings and then at the end apply dropout to everything. \n"
      ]
    },
    {
      "metadata": {
        "id": "b4ZzgTaku316",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, decoder_layer, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pf_dim = pf_dim\n",
        "        self.decoder_layer = decoder_layer\n",
        "        self.self_attention = self_attention\n",
        "        self.positionwise_feedforward = positionwise_feedforward\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([decoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        # trg = [batch_size, trg sent len]\n",
        "        # src = [batch_size, src sent len]\n",
        "        # trg_mask = [batch size, trg sent len]\n",
        "        # src_mask = [batch size, src sent len]\n",
        "        \n",
        "        pos = torch.arange(0, trg.shape[1]).unsqueeze(0).repeat(trg.shape[0], 1).to(self.device)\n",
        "        \n",
        "        # TODO \n",
        "        trg = self.do((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        # trg = [batch size, trg sent len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, src, trg_mask, src_mask)\n",
        "            \n",
        "        return self.fc(trg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Na143O3Qu317",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.ln = nn.LayerNorm(hid_dim)\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.ea = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        # trg = [batch size, trg sent len, hid dim]\n",
        "        # src = [batch size, src sent len, hid dim]\n",
        "        # trg_mask = [batch size, trg sent len]\n",
        "        # src_mask = [batch size, src sent len]\n",
        "                \n",
        "        trg = self.ln(trg + self.do(self.sa(trg, trg, trg, trg_mask)))\n",
        "        trg = self.ln(trg + self.do(self.ea(trg, src, src, src_mask)))\n",
        "        trg = self.ln(trg + self.do(self.pf(trg)))\n",
        "        \n",
        "        return trg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vz_hvERG8HeZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Seq2seq model itself doesn't change much. Yay modular code. "
      ]
    },
    {
      "metadata": {
        "id": "PbYQVjgru319",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_masks(self, src, trg):\n",
        "        # src = [batch size, src sent len]\n",
        "        # trg = [batch size, trg sent len]\n",
        "        \n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_pad_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), dtype=torch.uint8, device=self.device))\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        return src_mask, trg_mask\n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        # src = [batch size, src sent len]\n",
        "        # trg = [batch size, trg sent len]\n",
        "                \n",
        "        src_mask, trg_mask = self.make_masks(src, trg)\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        # enc_src = [batch size, src sent len, hid dim]\n",
        "                \n",
        "        out = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        # out = [batch size, trg sent len, output dim]\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wMLd_NlYu31_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_dim = len(SRC.vocab)\n",
        "hid_dim = 512\n",
        "n_layers = 6\n",
        "n_heads = 8\n",
        "pf_dim = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "enc = Encoder(input_dim, hid_dim, n_layers, n_heads, pf_dim, EncoderLayer, SelfAttention, PositionwiseFeedforward, dropout, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T3ZhVlC3u32C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_dim = len(TRG.vocab)\n",
        "hid_dim = 512\n",
        "n_layers = 6\n",
        "n_heads = 8\n",
        "pf_dim = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "dec = Decoder(output_dim, hid_dim, n_layers, n_heads, pf_dim, DecoderLayer, SelfAttention, PositionwiseFeedforward, dropout, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6X8jT8YRu32F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pad_idx = SRC.vocab.stoi['<pad>']\n",
        "\n",
        "model = Seq2Seq(enc, dec, pad_idx, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j1nExktMu32H",
        "colab_type": "code",
        "outputId": "e56a00e3-b74d-4b7f-daff-610ac5e0e96d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 55,205,125 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WazZy1w-u32L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "70rx05JY8OvK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A new optimzer is introduced in this paper. "
      ]
    },
    {
      "metadata": {
        "id": "qG0NA8hDu32M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itY96l3Ru32N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = NoamOpt(hid_dim, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kEbM3Z3ou32P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SkCfsy0eu32R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.optimizer.zero_grad()\n",
        "        output = model(src, trg[:,:-1])\n",
        "                \n",
        "        # output = [batch size, trg sent len - 1, output dim]\n",
        "        # trg = [batch size, trg sent len]\n",
        "        output = output.contiguous().view(-1, output.shape[-1])\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        # output = [batch size * trg sent len - 1, output dim]\n",
        "        # trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jfZR7IWzu32T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg[:,:-1])\n",
        "            # output = [batch size, trg sent len - 1, output dim]\n",
        "            # trg = [batch size, trg sent len]\n",
        "            output = output.contiguous().view(-1, output.shape[-1])\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            # output = [batch size * trg sent len - 1, output dim]\n",
        "            # trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "47z_qLBsu32U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8VoGPJj8VYh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TODO** Train for 5 epochs"
      ]
    },
    {
      "metadata": {
        "id": "df_eR_B2u32W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4ad86e02-241b-46e9-f3e8-c8e1aa21d853"
      },
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "SAVE_DIR = 'models'\n",
        "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'transformer-seq2seq.pt')\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "if not os.path.isdir(f'{SAVE_DIR}'):\n",
        "    os.makedirs(f'{SAVE_DIR}')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:03} | Time: {epoch_mins}m {epoch_secs}s| Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 001 | Time: 3m 21s| Train Loss: 5.948 | Train PPL: 383.051 | Val. Loss: 4.108 | Val. PPL:  60.820 |\n",
            "| Epoch: 002 | Time: 3m 28s| Train Loss: 3.771 | Train PPL:  43.420 | Val. Loss: 3.200 | Val. PPL:  24.543 |\n",
            "| Epoch: 003 | Time: 3m 28s| Train Loss: 3.132 | Train PPL:  22.909 | Val. Loss: 2.809 | Val. PPL:  16.598 |\n",
            "| Epoch: 004 | Time: 3m 27s| Train Loss: 2.762 | Train PPL:  15.834 | Val. Loss: 2.575 | Val. PPL:  13.128 |\n",
            "| Epoch: 005 | Time: 3m 27s| Train Loss: 2.503 | Train PPL:  12.218 | Val. Loss: 2.406 | Val. PPL:  11.095 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M4UNQFNru32Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "562d7e7e-0f11-4de8-adb8-dd613a430d40"
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 2.392 | Test PPL:  10.939 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8pdwU51Vzvep",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "\n",
        "Now that you have completed the assignment, follow the steps below to submit your aissgnment:\n",
        "1. Click __Runtime__  > __Run all__ to generate the output for all cells in the notebook.\n",
        "2. Save the notebook with the output from all the cells in the notebook by click __File__ > __Download .ipynb__.\n",
        "3. Copy model train and test prints, answers to all short questions, and the shareable link of this notebook to a `README.txt` file.\n",
        "4. Put the .ipynb file and `README.txt` under your hidden directory on the Zoo server `~/hidden/<YOUR_PIN>/Homework5/`.\n",
        "5. As a final step, run a script that will set up the permissions to your homework files, so we can access and run your code to grade it. Make sure the command runs without errors, and do not make any changes or run the code again. If you do run the code again or make any changes, you need to run the permissions script again. Submissions without the correct permissions may incur some grading penalty.\n",
        "`/home/classes/cs477/bash_files/hw5_set_permissions.sh <YOUR_PIN>`"
      ]
    },
    {
      "metadata": {
        "id": "Etx3-MGkzvvq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}